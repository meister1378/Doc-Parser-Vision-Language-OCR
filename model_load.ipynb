{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deed5fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1110 14:35:41.506570 1225972 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-10 14:35:43,031] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:35:43,032] [    INFO]\u001b[0m - Loading configuration file PaddleOCR-VL-SFT-test01/config.json\u001b[0m\n",
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/transformers/model_utils.py:394: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if re.search(f\"\\.{trans_key}\\.weight$\", key) or re.fullmatch(f\"^{trans_key}\\.weight$\", key):\n",
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/transformers/model_utils.py:394: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if re.search(f\"\\.{trans_key}\\.weight$\", key) or re.fullmatch(f\"^{trans_key}\\.weight$\", key):\n",
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/transformers/model_utils.py:394: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if re.search(f\"\\.{trans_key}\\.weight$\", key) or re.fullmatch(f\"^{trans_key}\\.weight$\", key):\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named model_state.pdparams, found in directory PaddleOCR-VL-SFT-test01.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m ckpt_dir = \u001b[33m\"\u001b[39m\u001b[33mPaddleOCR-VL-SFT-test01\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# 저장된 체크포인트 디렉토리\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 모델/토크나이저/이미지 프로세서 로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mPaddleOCRVLForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m tokenizer = Ernie4_5_Tokenizer.from_pretrained(ckpt_dir, padding_side=\u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m, model_max_length=\u001b[32m4096\u001b[39m)\n\u001b[32m     11\u001b[39m image_processor = SiglipImageProcessor.from_pretrained(ckpt_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/transformers/model_utils.py:2699\u001b[39m, in \u001b[36mPretrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[39m\n\u001b[32m   2696\u001b[39m use_keep_in_fp32_modules = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2698\u001b[39m \u001b[38;5;66;03m# resolve model_weight file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2699\u001b[39m resolved_archive_file, resolved_sharded_files, sharded_metadata, is_sharded = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve_model_file_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2702\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_hub\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_from_hf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_from_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2706\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2708\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2711\u001b[39m     \u001b[38;5;66;03m# 4. loading non-sharded ckpt from the state dict\u001b[39;00m\n\u001b[32m   2712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.tensor_parallel_degree > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file.endswith(\u001b[33m\"\u001b[39m\u001b[33mmodel_state.pdparams\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/transformers/model_utils.py:2002\u001b[39m, in \u001b[36mPretrainedModel._resolve_model_file_path\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, download_hub, cache_dir, subfolder, config, convert_from_hf, use_safetensors, variant)\u001b[39m\n\u001b[32m   1997\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1998\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(PYTORCH_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in directory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1999\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please set convert_from_hf=True in from_pretrained. eg, Model.from_pretrained(model_name, convert_from_hf=True) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2000\u001b[39m             )\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2002\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   2003\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(PADDLE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, found in directory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2004\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2005\u001b[39m         )\n\u001b[32m   2006\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_remote_url(pretrained_model_name_or_path):\n\u001b[32m   2007\u001b[39m     resolved_archive_file = resolve_file_path(\n\u001b[32m   2008\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   2009\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2012\u001b[39m         download_hub=download_hub,\n\u001b[32m   2013\u001b[39m     )\n",
      "\u001b[31mOSError\u001b[39m: Error no file named model_state.pdparams, found in directory PaddleOCR-VL-SFT-test01."
     ]
    }
   ],
   "source": [
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "ckpt_dir = \"PaddleOCR-VL-SFT-test01\"  # 저장된 체크포인트 디렉토리\n",
    "\n",
    "# 모델/토크나이저/이미지 프로세서 로드\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(ckpt_dir)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(ckpt_dir, padding_side=\"right\", model_max_length=4096)\n",
    "image_processor = SiglipImageProcessor.from_pretrained(ckpt_dir)\n",
    "\n",
    "# 입력 준비\n",
    "img = Image.open(\"/path/to/your/image.jpg\").convert(\"RGB\")\n",
    "prompt = \"다음 이미지의 텍스트를 읽어줘.\"\n",
    "\n",
    "# 간단한 전처리/토크나이즈 (프로젝트 파이프라인과 동일하게 맞추면 가장 좋습니다)\n",
    "inputs = {\n",
    "    \"context\": [\n",
    "        {\"role\": \"user\", \"utterance\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": img, \"image_width\": img.width, \"image_height\": img.height}},\n",
    "        ]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 프로젝트의 End2EndProcessor를 사용할 수도 있습니다.\n",
    "from data_processor.steps.end2end_processing import End2EndProcessor\n",
    "processor = End2EndProcessor(\n",
    "    args=type(\"A\", (), {\"batch_size\":1, \"load_args_from_api\":False, \"max_seq_length\":4096, \"im_prefix_length\":64,\n",
    "                        \"variable_resolution\":1, \"spatial_conv_size\":1, \"adaptive_max_imgtoken_option\":None,\n",
    "                        \"adaptive_max_imgtoken_rate\":None, \"video_min_pixels\":None, \"video_max_pixels\":None,\n",
    "                        \"drop_untrainble_sample\":False, \"chat_template\":\"ernie_vl\"})(),\n",
    "    tokenizer=tokenizer,\n",
    "    image_preprocess=image_processor\n",
    ")\n",
    "features = processor.process(inputs)[0]\n",
    "\n",
    "# 모델 포워드 (loss 없이 디코딩/생성)\n",
    "model.eval()\n",
    "with paddle.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=paddle.to_tensor([features[\"ds16\"]], dtype=\"int64\"),\n",
    "        token_type_ids=paddle.to_tensor([features[\"ds16_tokenwise_type_id\"]], dtype=\"int64\"),\n",
    "        image_type_ids=paddle.to_tensor([features[\"ds16_imagewise_type_id\"]], dtype=\"int64\"),\n",
    "        images=features[\"meta\"][0][\"images\"],  # 프로젝트 내부 포맷에 맞춰 전달\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "text = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57fdad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meister1378/vl-lora'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c08c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee4e8246db43f78530952755ce2c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/509 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8170fe31626e45b38885893ce2da46f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "meister1378/vl-lora does not appear to have a file named processing_ppocrvl.py. Checkout 'https://huggingface.co/meister1378/vl-lora/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoProcessor\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:376\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m    372\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m    373\u001b[39m     )\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     processor_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessor_auto_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    380\u001b[39m     processor_class.register_for_auto_class()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:570\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m     code_revision = revision\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload=force_download)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:372\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m new_files = []\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     resolved_module_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local \u001b[38;5;129;01mand\u001b[39;00m cached_module != resolved_module_file:\n\u001b[32m    386\u001b[39m         new_files.append(module_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/utils/hub.py:583\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    579\u001b[39m     revision_ = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m revision\n\u001b[32m    580\u001b[39m     msg = (\n\u001b[32m    581\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_entries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_entries) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfiles named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(*missing_entries,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    584\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m     )\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# Remove potential missing entries (we can silently remove them at this point based on the flags)\u001b[39;00m\n\u001b[32m    589\u001b[39m resolved_files = [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mOSError\u001b[39m: meister1378/vl-lora does not appear to have a file named processing_ppocrvl.py. Checkout 'https://huggingface.co/meister1378/vl-lora/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e78185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-10 14:45:06,008] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:06,009] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:06,011] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,632] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,774] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,778] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,782] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,791] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,796] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,800] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,809] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,814] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,820] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,828] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,832] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,842] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,846] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,855] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,859] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,900] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,908] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:45:59,913] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:46:00,371] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:46:00,372] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:46:00,374] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:46:00,376] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-10 14:46:00,453] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:46:00,506] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-10 14:46:00,734] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "base_model = \"PaddlePaddle/PaddleOCR-VL\"  # 학습 시 사용한 베이스와 동일해야 함\n",
    "lora_dir   = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\"  # peft_model*.safetensors 위치\n",
    "\n",
    "# 1) 베이스 모델을 HF→Paddle 변환 로딩\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(\n",
    "    base_model,\n",
    "    convert_from_hf=True,      # 핵심: HF 포맷을 Paddle 형태로 변환\n",
    ")\n",
    "\n",
    "# 2) 토크나이저/이미지 프로세서 (베이스와 동일 소스 권장)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(\n",
    "    base_model, padding_side=\"right\", model_max_length=4096\n",
    ")\n",
    "image_preprocessor = SiglipImageProcessor.from_pretrained(base_model)\n",
    "\n",
    "# 3) LoRA 어댑터 적용 (출력 디렉토리에서)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=lora_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03020e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1110 15:18:24.859114 1255067 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-10 15:18:26,354] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:26,355] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:26,356] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,308] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,524] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,529] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,536] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,548] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,554] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,564] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,569] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,579] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,583] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,593] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,598] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,603] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,611] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,617] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,626] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,633] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,642] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:30,646] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,162] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,163] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,167] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,167] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-10 15:18:31,179] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,221] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,343] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,488] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 15:18:31,489] [    INFO]\u001b[0m - Image processor saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged/preprocessor_config.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged/preprocessor_config.json']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "base_model = \"PaddlePaddle/PaddleOCR-VL\"  # 학습 시 사용한 베이스와 동일해야 함\n",
    "lora_dir   = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\"  # peft_model*.safetensors 위치\n",
    "hf_out_dir = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged\"\n",
    "\n",
    "# 1) 베이스 모델을 HF→Paddle 변환 로딩\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(\n",
    "    base_model,\n",
    "    convert_from_hf=True,      # 핵심: HF 포맷을 Paddle 형태로 변환\n",
    ")\n",
    "\n",
    "# 2) 토크나이저/이미지 프로세서 (베이스와 동일 소스 권장)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(\n",
    "    base_model, padding_side=\"right\", model_max_length=8192\n",
    ")\n",
    "image_preprocessor = SiglipImageProcessor.from_pretrained(base_model)\n",
    "\n",
    "# 3) LoRA 어댑터 적용 (출력 디렉토리에서)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=lora_dir)\n",
    "\n",
    "model.save_pretrained(hf_out_dir, safe_serialization=True)\n",
    "\n",
    "image_preprocessor.save_pretrained(hf_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e24e9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1110 18:12:57.320622 1407321 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-10 18:12:58,982] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:12:58,983] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:12:58,985] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,142] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,309] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,316] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,321] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,325] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,333] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,339] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,343] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,352] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,357] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,364] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,370] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,376] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,384] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,390] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,395] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,403] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,408] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,415] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,885] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,886] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,888] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,889] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-10 18:13:03,900] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:03,942] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-10 18:13:04,100] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "# 1) 경로 설정\n",
    "base_model = \"PaddlePaddle/PaddleOCR-VL\"  # 베이스 모델 허브 경로(또는 로컬 경로)\n",
    "lora_dir   = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test03\"  # peft_model*.safetensors 있는 디렉터리\n",
    "\n",
    "# 2) 베이스 모델/토크나이저/이미지 프로세서 로드\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(base_model, convert_from_hf=True)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(base_model, padding_side=\"right\", model_max_length=8192)\n",
    "image_preprocessor = SiglipImageProcessor.from_pretrained(base_model)\n",
    "\n",
    "# 3) LoRA 어댑터 적용 (출력 디렉터리에서)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=lora_dir)\n",
    "#model.eval()\n",
    "\n",
    "# (선택) LoRA 병합 후 고정이 필요하면, 다음 API가 지원되면 사용:\n",
    "# model.merge_and_unload()  # 지원되는 경우에만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd94661",
   "metadata": {},
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-6911988b-224df3f3516a769c77a14a45;85cd3b56-412b-4e1a-a5f6-7fa8a8120fab)\n\nRepository Not Found for url: https://huggingface.co/api/models/meister1378/vl-lora/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/api/models/meister1378/vl-lora/preupload/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m      2\u001b[39m api = HfApi()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeister1378/vl-lora\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1687\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1684\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1686\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1687\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4930\u001b[39m, in \u001b[36mHfApi.upload_folder\u001b[39m\u001b[34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[39m\n\u001b[32m   4926\u001b[39m commit_operations = delete_operations + add_operations\n\u001b[32m   4928\u001b[39m commit_message = commit_message \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUpload folder using huggingface_hub\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4930\u001b[39m commit_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[32m   4943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info.pr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1687\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1684\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1686\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1687\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4229\u001b[39m, in \u001b[36mHfApi.create_commit\u001b[39m\u001b[34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[39m\n\u001b[32m   4226\u001b[39m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[32m   4227\u001b[39m _warn_on_overwriting_operations(operations)\n\u001b[32m-> \u001b[39m\u001b[32m4229\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4231\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[32m   4235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[32m   4238\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4240\u001b[39m files_to_copy = _fetch_files_to_copy(\n\u001b[32m   4241\u001b[39m     copies=copies,\n\u001b[32m   4242\u001b[39m     repo_type=repo_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4246\u001b[39m     endpoint=\u001b[38;5;28mself\u001b[39m.endpoint,\n\u001b[32m   4247\u001b[39m )\n\u001b[32m   4248\u001b[39m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4449\u001b[39m, in \u001b[36mHfApi.preupload_lfs_files\u001b[39m\u001b[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[39m\n\u001b[32m   4447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(additions_no_upload_mode) > \u001b[32m0\u001b[39m:\n\u001b[32m   4448\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4449\u001b[39m         \u001b[43m_fetch_upload_modes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4450\u001b[39m \u001b[43m            \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditions_no_upload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4451\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4452\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4453\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4454\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4455\u001b[39m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4456\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4457\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4458\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4459\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4460\u001b[39m         e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/_commit_api.py:752\u001b[39m, in \u001b[36m_fetch_upload_modes\u001b[39m\u001b[34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001b[39m\n\u001b[32m    744\u001b[39m     payload[\u001b[33m\"\u001b[39m\u001b[33mgitIgnore\u001b[39m\u001b[33m\"\u001b[39m] = gitignore_content\n\u001b[32m    746\u001b[39m resp = get_session().post(\n\u001b[32m    747\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/preupload/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    748\u001b[39m     json=payload,\n\u001b[32m    749\u001b[39m     headers=headers,\n\u001b[32m    750\u001b[39m     params={\u001b[33m\"\u001b[39m\u001b[33mcreate_pr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    751\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    753\u001b[39m preupload_info = _validate_preupload_info(resp.json())\n\u001b[32m    754\u001b[39m upload_modes.update(**{file[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m]: file[\u001b[33m\"\u001b[39m\u001b[33muploadMode\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m preupload_info[\u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m]})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:452\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mRepoNotFound\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    432\u001b[39m     response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m    433\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m error_message != \u001b[33m\"\u001b[39m\u001b[33mInvalid credentials in Authorization header\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[32m    443\u001b[39m     message = (\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m    455\u001b[39m     message = (\n\u001b[32m    456\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m endpoint:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBad request:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-6911988b-224df3f3516a769c77a14a45;85cd3b56-412b-4e1a-a5f6-7fa8a8120fab)\n\nRepository Not Found for url: https://huggingface.co/api/models/meister1378/vl-lora/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"/home/mango/ERNIE/PaddleOCR-VL-SFT-test03\",\n",
    "    repo_id=\"meister1378/vl-lora\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ac22bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-10 16:47:42,864] [    INFO]\u001b[0m - Configuration saved in /tmp/tmph29pu8a1/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 16:47:42,871] [    INFO]\u001b[0m - Configuration saved in /tmp/tmph29pu8a1/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 16:47:48,286] [    INFO]\u001b[0m - Model weights saved in /tmp/tmph29pu8a1/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-11-10 16:47:48,287] [    INFO]\u001b[0m - README.md not found, adding the default README.md\u001b[0m\n",
      "\u001b[32m[2025-11-10 16:47:48,288] [    INFO]\u001b[0m - Pushing to the meister1378/vl-lora. This might take a while\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fd8c118943413189ed524c01b298db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456c31d9a76a46a6b53b1274e7ba55ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/meister1378/vl-lora/commit/dc2b278a96ca326e78b62b4647aa0395336064dc', commit_message='upload lora adapter', commit_description='', oid='dc2b278a96ca326e78b62b4647aa0395336064dc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/meister1378/vl-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='meister1378/vl-lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_to_hf_hub(repo_id=\"meister1378/vl-lora\", commit_message=\"upload lora adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1956125b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db05978b82047799cc3aa4b204901a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6761f93271c34ee389661010c454e23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/meister1378/vl-lora/commit/685d5f6e171caeb04ba92dbab70d131b3a9d0a33', commit_message='Upload folder using huggingface_hub', commit_description='', oid='685d5f6e171caeb04ba92dbab70d131b3a9d0a33', pr_url=None, repo_url=RepoUrl('https://huggingface.co/meister1378/vl-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='meister1378/vl-lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\",\n",
    "    repo_id=\"meister1378/vl-lora\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The repository meister1378/vl-lora contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/meister1378/vl-lora .\n You can inspect the repository content at https://hf.co/meister1378/vl-lora.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoProcessor\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeister1378/vl-lora\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:547\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    545\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1259\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1258\u001b[39m         upstream_repo = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupstream_repo\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m   1264\u001b[39m     config_class = get_class_from_dynamic_module(\n\u001b[32m   1265\u001b[39m         class_ref, pretrained_model_name_or_path, code_revision=code_revision, **kwargs\n\u001b[32m   1266\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:748\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code, error_message, upstream_repo)\u001b[39m\n\u001b[32m    745\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    749\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m You can inspect the repository content at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    750\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    751\u001b[39m     )\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: The repository meister1378/vl-lora contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/meister1378/vl-lora .\n You can inspect the repository content at https://hf.co/meister1378/vl-lora.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meister1378/vl-lora\", resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b5ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-10 17:20:54,650] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,651] [    INFO]\u001b[0m - Loading configuration file /root/.cache/huggingface/hub/models--meister1378--vl-lora/snapshots/685d5f6e171caeb04ba92dbab70d131b3a9d0a33/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,652] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,653] [    INFO]\u001b[0m - Loading weights file from cache at /root/.cache/huggingface/hub/models--meister1378--vl-lora/snapshots/685d5f6e171caeb04ba92dbab70d131b3a9d0a33/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,654] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,715] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,718] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,724] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,727] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,729] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,732] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,734] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,737] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,739] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,741] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,743] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,746] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,753] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,757] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,760] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,765] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,770] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,773] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,785] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[33m[2025-11-10 17:20:54,787] [ WARNING]\u001b[0m - Some weights of PaddleOCRVLForConditionalGeneration were not initialized from the model checkpoint at meister1378/vl-lora and are newly initialized: ['visual.vision_model.encoder.layers.15.layer_norm2.bias', 'model.layers.14.input_layernorm.weight', 'visual.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.3.layer_norm2.bias', 'visual.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.layers.15.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.20.layer_norm2.bias', 'visual.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.7.mlp.fc2.bias', 'visual.vision_model.encoder.layers.8.mlp.fc2.weight', 'visual.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.12.mlp.fc2.bias', 'visual.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'model.layers.14.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.8.mlp.fc2.bias', 'visual.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.26.mlp.fc1.bias', 'visual.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'mlp_AR.pre_norm.bias', 'visual.vision_model.encoder.layers.15.mlp.fc2.bias', 'visual.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.11.layer_norm1.bias', 'model.norm.weight', 'visual.vision_model.encoder.layers.19.mlp.fc1.bias', 'visual.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.2.mlp.fc2.weight', 'visual.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.23.mlp.fc1.weight', 'visual.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.5.mlp.fc2.bias', 'visual.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.layers.6.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.0.layer_norm1.weight', 'visual.vision_model.encoder.layers.20.mlp.fc1.weight', 'visual.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.21.mlp.fc2.bias', 'visual.vision_model.encoder.layers.24.layer_norm1.bias', 'visual.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.2.layer_norm2.weight', 'model.layers.4.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.18.mlp.fc2.bias', 'visual.vision_model.encoder.layers.13.layer_norm2.weight', 'visual.vision_model.encoder.layers.23.mlp.fc2.bias', 'visual.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.22.mlp.fc2.bias', 'mlp_AR.pre_norm.weight', 'visual.vision_model.encoder.layers.8.mlp.fc1.bias', 'visual.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.11.mlp.fc2.weight', 'visual.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.12.layer_norm2.weight', 'visual.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.13.layer_norm1.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.22.mlp.fc2.weight', 'visual.vision_model.embeddings.position_embedding.weight', 'visual.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'visual.vision_model.embeddings.patch_embedding.weight', 'visual.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'visual.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.layers.15.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.18.layer_norm1.weight', 'model.layers.1.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.layers.11.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.layers.10.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.2.layer_norm1.bias', 'model.layers.1.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.14.layer_norm1.weight', 'visual.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.layers.5.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.18.mlp.fc1.weight', 'visual.vision_model.encoder.layers.26.mlp.fc2.bias', 'model.layers.3.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.12.layer_norm1.bias', 'visual.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'visual.vision_model.head.mlp.fc2.bias', 'visual.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.layers.15.input_layernorm.weight', 'visual.vision_model.encoder.layers.16.mlp.fc2.bias', 'visual.vision_model.encoder.layers.17.layer_norm2.bias', 'visual.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.15.layer_norm2.weight', 'visual.vision_model.head.attention.in_proj_weight', 'visual.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.20.layer_norm1.bias', 'model.layers.15.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.4.layer_norm1.bias', 'model.layers.6.mlp.down_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.12.mlp.fc1.weight', 'visual.vision_model.encoder.layers.2.mlp.fc1.bias', 'visual.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.7.mlp.fc2.weight', 'visual.vision_model.encoder.layers.15.mlp.fc2.weight', 'visual.vision_model.encoder.layers.3.mlp.fc1.bias', 'visual.vision_model.encoder.layers.20.layer_norm1.weight', 'visual.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'mlp_AR.linear_2.weight', 'model.layers.1.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.14.layer_norm2.weight', 'visual.vision_model.encoder.layers.5.mlp.fc2.weight', 'visual.vision_model.encoder.layers.8.layer_norm2.weight', 'visual.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.25.mlp.fc1.weight', 'visual.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'visual.vision_model.head.attention.in_proj_bias', 'visual.vision_model.encoder.layers.15.mlp.fc1.weight', 'visual.vision_model.encoder.layers.22.mlp.fc1.weight', 'visual.vision_model.encoder.layers.8.layer_norm1.weight', 'visual.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.layers.2.input_layernorm.weight', 'visual.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.7.layer_norm2.bias', 'visual.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.25.mlp.fc2.weight', 'model.layers.12.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.3.layer_norm1.weight', 'visual.vision_model.encoder.layers.24.mlp.fc1.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.3.mlp.fc1.weight', 'visual.vision_model.encoder.layers.22.mlp.fc1.bias', 'visual.vision_model.encoder.layers.25.layer_norm1.bias', 'visual.vision_model.encoder.layers.19.mlp.fc2.bias', 'visual.vision_model.encoder.layers.22.layer_norm2.bias', 'visual.vision_model.encoder.layers.21.layer_norm2.weight', 'lm_head.weight', 'mlp_AR.linear_1.bias', 'model.layers.12.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.12.layer_norm1.weight', 'visual.vision_model.encoder.layers.4.mlp.fc2.bias', 'visual.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.layers.2.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.12.mlp.fc1.bias', 'visual.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.0.mlp.fc1.bias', 'visual.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.24.layer_norm2.weight', 'visual.vision_model.encoder.layers.26.mlp.fc1.weight', 'model.layers.7.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.16.layer_norm1.weight', 'model.layers.13.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.2.layer_norm1.weight', 'visual.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.25.layer_norm2.weight', 'visual.vision_model.encoder.layers.5.layer_norm1.bias', 'visual.vision_model.encoder.layers.23.mlp.fc2.weight', 'visual.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.16.mlp.fc1.weight', 'visual.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.23.layer_norm1.weight', 'model.layers.3.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.1.mlp.fc2.bias', 'visual.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.6.mlp.fc1.weight', 'visual.vision_model.encoder.layers.18.layer_norm2.weight', 'visual.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.7.layer_norm1.weight', 'visual.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.layers.5.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.3.layer_norm2.weight', 'visual.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.5.layer_norm2.bias', 'visual.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.layers.8.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.0.layer_norm1.bias', 'visual.vision_model.encoder.layers.18.mlp.fc1.bias', 'mlp_AR.linear_2.bias', 'visual.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.21.mlp.fc1.bias', 'visual.vision_model.encoder.layers.13.layer_norm2.bias', 'visual.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'model.layers.8.input_layernorm.weight', 'model.layers.16.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.10.layer_norm1.bias', 'visual.vision_model.encoder.layers.25.layer_norm1.weight', 'model.layers.16.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.0.mlp.fc2.bias', 'visual.vision_model.encoder.layers.10.mlp.fc2.weight', 'visual.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.10.layer_norm2.weight', 'visual.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.8.layer_norm2.bias', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.17.mlp.fc2.weight', 'visual.vision_model.encoder.layers.16.layer_norm2.bias', 'visual.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.17.layer_norm1.bias', 'visual.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.6.layer_norm2.weight', 'visual.vision_model.encoder.layers.18.mlp.fc2.weight', 'visual.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.layers.2.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.22.layer_norm1.bias', 'visual.vision_model.encoder.layers.11.mlp.fc1.weight', 'visual.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.6.layer_norm1.weight', 'visual.vision_model.encoder.layers.16.layer_norm1.bias', 'visual.vision_model.embeddings.packing_position_embedding.weight', 'visual.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.3.layer_norm1.bias', 'visual.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.9.layer_norm2.weight', 'visual.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.layers.6.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'visual.vision_model.head.attention.out_proj.bias', 'visual.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'visual.vision_model.head.attention.out_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'visual.vision_model.head.probe', 'visual.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.layers.10.input_layernorm.weight', 'visual.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.layers.12.input_layernorm.weight', 'visual.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.7.mlp.fc1.bias', 'visual.vision_model.encoder.layers.26.layer_norm1.weight', 'model.layers.2.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.20.layer_norm2.weight', 'visual.vision_model.encoder.layers.26.layer_norm1.bias', 'visual.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.6.layer_norm1.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.18.layer_norm2.bias', 'mlp_AR.linear_1.weight', 'visual.vision_model.encoder.layers.6.mlp.fc2.weight', 'visual.vision_model.encoder.layers.5.mlp.fc1.bias', 'visual.vision_model.encoder.layers.24.mlp.fc2.weight', 'visual.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.3.mlp.fc2.weight', 'visual.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.layers.3.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.12.layer_norm2.bias', 'model.layers.2.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.layers.1.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.2.mlp.fc2.bias', 'visual.vision_model.encoder.layers.2.layer_norm2.bias', 'visual.vision_model.encoder.layers.19.layer_norm2.bias', 'visual.vision_model.encoder.layers.1.layer_norm1.bias', 'model.layers.1.input_layernorm.weight', 'visual.vision_model.encoder.layers.17.mlp.fc1.bias', 'visual.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.23.layer_norm2.weight', 'visual.vision_model.encoder.layers.13.mlp.fc1.weight', 'visual.vision_model.encoder.layers.10.mlp.fc1.weight', 'visual.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.layers.15.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.11.layer_norm2.bias', 'visual.vision_model.post_layernorm.bias', 'visual.vision_model.encoder.layers.11.mlp.fc1.bias', 'visual.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'visual.vision_model.embeddings.patch_embedding.bias', 'visual.vision_model.head.mlp.fc1.weight', 'visual.vision_model.encoder.layers.20.mlp.fc2.weight', 'visual.vision_model.encoder.layers.11.layer_norm2.weight', 'visual.vision_model.encoder.layers.19.layer_norm2.weight', 'visual.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.18.layer_norm1.bias', 'visual.vision_model.encoder.layers.4.layer_norm2.weight', 'model.layers.10.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.0.mlp.fc1.weight', 'visual.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.5.mlp.fc1.weight', 'visual.vision_model.encoder.layers.1.layer_norm2.bias', 'visual.vision_model.encoder.layers.13.layer_norm1.bias', 'visual.vision_model.encoder.layers.10.layer_norm1.weight', 'model.layers.4.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.12.mlp.fc2.weight', 'visual.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.layers.7.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.0.layer_norm2.weight', 'model.layers.8.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.6.mlp.fc2.bias', 'visual.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.14.mlp.fc1.weight', 'visual.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.11.layer_norm1.weight', 'visual.vision_model.encoder.layers.19.layer_norm1.bias', 'visual.vision_model.encoder.layers.23.layer_norm2.bias', 'visual.vision_model.encoder.layers.15.layer_norm1.bias', 'model.layers.7.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'visual.vision_model.head.mlp.fc1.bias', 'model.layers.9.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.5.layer_norm1.weight', 'visual.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.0.layer_norm2.bias', 'model.layers.2.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.7.layer_norm1.bias', 'visual.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.layers.0.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.9.layer_norm2.bias', 'model.layers.11.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.7.layer_norm2.weight', 'visual.vision_model.encoder.layers.0.mlp.fc2.weight', 'visual.vision_model.encoder.layers.14.layer_norm1.bias', 'visual.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.layers.5.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.24.mlp.fc1.weight', 'visual.vision_model.head.mlp.fc2.weight', 'visual.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.layers.9.input_layernorm.weight', 'visual.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.13.mlp.fc2.bias', 'visual.vision_model.encoder.layers.1.mlp.fc1.weight', 'visual.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.8.layer_norm1.bias', 'model.layers.7.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.16.mlp.fc1.bias', 'visual.vision_model.encoder.layers.1.layer_norm1.weight', 'visual.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.9.mlp.fc1.weight', 'visual.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.3.mlp.fc2.bias', 'visual.vision_model.encoder.layers.15.layer_norm1.weight', 'visual.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.layers.12.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.24.layer_norm2.bias', 'visual.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.13.mlp.fc2.weight', 'visual.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.4.layer_norm2.bias', 'visual.vision_model.encoder.layers.17.layer_norm1.weight', 'visual.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.25.mlp.fc2.bias', 'visual.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.6.layer_norm2.bias', 'model.layers.10.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.layers.13.input_layernorm.weight', 'visual.vision_model.encoder.layers.17.layer_norm2.weight', 'visual.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.layers.8.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.24.mlp.fc2.bias', 'visual.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.26.mlp.fc2.weight', 'visual.vision_model.post_layernorm.weight', 'visual.vision_model.encoder.layers.23.layer_norm1.bias', 'model.layers.11.input_layernorm.weight', 'visual.vision_model.encoder.layers.21.layer_norm1.weight', 'model.layers.13.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.19.mlp.fc2.weight', 'visual.vision_model.head.layernorm.bias', 'visual.vision_model.encoder.layers.26.layer_norm2.bias', 'visual.vision_model.encoder.layers.22.layer_norm2.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'visual.vision_model.head.layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'visual.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.layers.15.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.layers.17.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.21.layer_norm2.bias', 'model.embed_tokens.weight', 'visual.vision_model.encoder.layers.16.layer_norm2.weight', 'visual.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.4.mlp.fc1.bias', 'visual.vision_model.encoder.layers.9.layer_norm1.weight', 'visual.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.9.layer_norm1.bias', 'visual.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.25.layer_norm2.bias', 'visual.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'visual.vision_model.encoder.layers.23.mlp.fc1.bias', 'visual.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.layers.6.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'visual.vision_model.encoder.layers.25.mlp.fc1.bias', 'visual.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.4.input_layernorm.weight', 'visual.vision_model.encoder.layers.10.layer_norm2.bias', 'visual.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.24.layer_norm1.weight', 'visual.vision_model.encoder.layers.17.mlp.fc2.bias', 'visual.vision_model.encoder.layers.5.layer_norm2.weight', 'visual.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.14.layer_norm2.bias', 'visual.vision_model.encoder.layers.1.layer_norm2.weight', 'visual.vision_model.encoder.layers.19.layer_norm1.weight', 'visual.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.layers.14.self_attn.o_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.21.layer_norm1.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.4.layer_norm1.weight', 'visual.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.layers.7.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.26.layer_norm2.weight', 'visual.vision_model.encoder.layers.17.mlp.fc1.weight', 'visual.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.22.layer_norm1.weight', 'model.layers.17.input_layernorm.weight', 'visual.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.10.mlp.fc1.bias', 'visual.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.layers.6.input_layernorm.weight', 'visual.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,792] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 17:20:54,794] [    INFO]\u001b[0m - Loading configuration file /root/.cache/huggingface/hub/models--meister1378--vl-lora/snapshots/685d5f6e171caeb04ba92dbab70d131b3a9d0a33/generation_config.json\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "meister1378/vl-lora does not appear to have a file named processing_ppocrvl.py. Checkout 'https://huggingface.co/meister1378/vl-lora/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m image = Image.open(image_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m model = PaddleOCRVLForConditionalGeneration.from_pretrained(model_path, convert_from_hf=\u001b[38;5;28;01mTrue\u001b[39;00m).eval()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m messages = [\n\u001b[32m     27\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,         \n\u001b[32m     28\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     }\n\u001b[32m     33\u001b[39m ]\n\u001b[32m     34\u001b[39m inputs = processor.apply_chat_template(\n\u001b[32m     35\u001b[39m     messages, \n\u001b[32m     36\u001b[39m     tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m ).to(DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:376\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m    372\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m    373\u001b[39m     )\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     processor_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessor_auto_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    380\u001b[39m     processor_class.register_for_auto_class()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:570\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m     code_revision = revision\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload=force_download)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:372\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m new_files = []\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     resolved_module_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local \u001b[38;5;129;01mand\u001b[39;00m cached_module != resolved_module_file:\n\u001b[32m    386\u001b[39m         new_files.append(module_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/utils/hub.py:583\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    579\u001b[39m     revision_ = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m revision\n\u001b[32m    580\u001b[39m     msg = (\n\u001b[32m    581\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_entries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_entries) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfiles named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(*missing_entries,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    584\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m     )\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# Remove potential missing entries (we can silently remove them at this point based on the flags)\u001b[39;00m\n\u001b[32m    589\u001b[39m resolved_files = [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mOSError\u001b[39m: meister1378/vl-lora does not appear to have a file named processing_ppocrvl.py. Checkout 'https://huggingface.co/meister1378/vl-lora/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CHOSEN_TASK = \"ocr\"  # Options: 'ocr' | 'table' | 'chart' | 'formula'\n",
    "PROMPTS = {\n",
    "    \"ocr\": \"OCR:\",\n",
    "    \"table\": \"Table Recognition:\",\n",
    "    \"formula\": \"Formula Recognition:\",\n",
    "    \"chart\": \"Chart Recognition:\",\n",
    "}\n",
    "\n",
    "model_path = \"meister1378/vl-lora\"\n",
    "image_path = \"000000014_경남.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(model_path, convert_from_hf=True).eval()\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "messages = [\n",
    "    {\"role\": \"user\",         \n",
    "     \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": PROMPTS[CHOSEN_TASK]},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True, \t\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "outputs_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "outputs = processor.batch_decode(outputs_ids, skip_special_tokens=False)[0]\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc97c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PaddleOCRVLForConditionalGeneration' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/peft/lora/lora_model.py:751\u001b[39m, in \u001b[36mLoRAModel.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Layer's logic\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/nn/layer/layers.py:1798\u001b[39m, in \u001b[36mLayer.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _buffers[name]\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'LoRAModel' object has no attribute 'save'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m(safe_serialization=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/peft/lora/lora_model.py:753\u001b[39m, in \u001b[36mLoRAModel.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattr__\u001b[39m(name)  \u001b[38;5;66;03m# defer to nn.Layer's logic\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/nn/layer/layers.py:1798\u001b[39m, in \u001b[36mLayer.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1796\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m _convert_into_variable(_buffers[name])\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _buffers[name]\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PaddleOCRVLForConditionalGeneration' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save(safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ff4b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 베이스 모델 로딩...\n",
      "2. LoRA 어댑터 적용...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['do_qat', 'dtype', 'enable_lora_list', 'head_dim', 'lora_plus_scale', 'lora_use_mixer', 'loraga', 'lorapro', 'merge_weights', 'pissa', 'rslora', 'scaling', 'tensor_parallel_degree', 'trainable_bias', 'trainable_modules', 'use_mora', 'use_quick_lora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'.*o_proj.*', '.*spatial_linear.2.*', '.*temporal_linear.2.*', '.*down_proj.*', '.*temporal_linear.0.*', '.*spatial_linear.0.*', '.*up_gate_proj.*', '.*qkv_proj.*'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m2. LoRA 어댑터 적용...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 3. LoRA 어댑터 적용\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 첫 번째 인자로 로드된 `base_model` 객체를 전달합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_adapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m3. 어댑터와 베이스 모델 병합...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 4. LoRA 가중치를 베이스 모델에 완전히 병합하고, 어댑터는 메모리에서 해제\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/peft_model.py:547\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    540\u001b[39m         model,\n\u001b[32m    541\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    545\u001b[39m     )\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     model = \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m load_result = model.load_adapter(\n\u001b[32m    556\u001b[39m     model_id,\n\u001b[32m    557\u001b[39m     adapter_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     **kwargs,\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/peft_model.py:1815\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1814\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1816\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/peft_model.py:130\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    128\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    134\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:209\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:654\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    653\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[32m    657\u001b[39m     error_msg = (\n\u001b[32m    658\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo modules were targeted for adaptation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check your `target_modules` and `exclude_modules` configuration. You may also have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly targeted modules that are marked to be saved (`modules_to_save`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    662\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Target modules {'.*o_proj.*', '.*spatial_linear.2.*', '.*temporal_linear.2.*', '.*down_proj.*', '.*temporal_linear.0.*', '.*spatial_linear.0.*', '.*up_gate_proj.*', '.*qkv_proj.*'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 (최신 버전 권장)\n",
    "# !pip install -U transformers peft accelerate safetensors torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. 경로 설정\n",
    "base_model_path = \"PaddlePaddle/PaddleOCR-VL\"\n",
    "lora_adapter_path = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\"\n",
    "merged_model_save_path = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged\"\n",
    "\n",
    "print(\"1. 베이스 모델 로딩...\")\n",
    "# 2. 베이스 모델 로드\n",
    "# trust_remote_code=True는 허깅페이스에 등록되지 않은 커스텀 모델 코드를 실행하도록 허용합니다.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16, # 메모리 사용량을 줄이기 위해 float16 사용 (GPU 환경)\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"2. LoRA 어댑터 적용...\")\n",
    "# 3. LoRA 어댑터 적용\n",
    "# 첫 번째 인자로 로드된 `base_model` 객체를 전달합니다.\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "\n",
    "print(\"3. 어댑터와 베이스 모델 병합...\")\n",
    "# 4. LoRA 가중치를 베이스 모델에 완전히 병합하고, 어댑터는 메모리에서 해제\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(f\"4. 병합된 모델을 '{merged_model_save_path}'에 저장...\")\n",
    "# 5. 병합된 모델 저장\n",
    "model.save_pretrained(merged_model_save_path, safe_serialization=True)\n",
    "\n",
    "print(f\"5. 프로세서(Processor)를 '{merged_model_save_path}'에 저장...\")\n",
    "# 6. 프로세서(이미지/텍스트 전처리기)도 함께 저장해야 나중에 모델을 쉽게 불러올 수 있습니다.\n",
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "processor.save_pretrained(merged_model_save_path)\n",
    "\n",
    "print(\"\\n작업 완료! 이제부터는 아래와 같이 병합된 모델을 직접 로드할 수 있습니다.\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(f\"from transformers import AutoModelForCausalLM, AutoProcessor\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{merged_model_save_path}', trust_remote_code=True)\")\n",
    "print(f\"processor = AutoProcessor.from_pretrained('{merged_model_save_path}', trust_remote_code=True)\")\n",
    "print(\"--------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03a7747",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '/home/mango/ERNIE/PaddleOCR-VL-SFT-test01'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/config.py:262\u001b[39m, in \u001b[36mPeftConfigMixin._get_peft_type\u001b[39m\u001b[34m(cls, model_id, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     config_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/mango/ERNIE/PaddleOCR-VL-SFT-test01'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m hf_out_dir = \u001b[33m\"\u001b[39m\u001b[33m/home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m pre_model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_dir\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 어댑터 로드\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#model = model.merge_and_unload()                     # LoRA 병합\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# model.save_pretrained(hf_out_dir, safe_serialization=True)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# processor.save_pretrained(hf_out_dir)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/peft_model.py:440\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    439\u001b[39m     config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m         \u001b[43mPeftConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubfolder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrevision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_auth_token\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     ].from_pretrained(model_id, **kwargs)\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[32m    450\u001b[39m     config.inference_mode = \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/peft/config.py:268\u001b[39m, in \u001b[36mPeftConfigMixin._get_peft_type\u001b[39m\u001b[34m(cls, model_id, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    262\u001b[39m         config_file = hf_hub_download(\n\u001b[32m    263\u001b[39m             model_id,\n\u001b[32m    264\u001b[39m             CONFIG_NAME,\n\u001b[32m    265\u001b[39m             **hf_hub_download_kwargs,\n\u001b[32m    266\u001b[39m         )\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m loaded_attributes = \u001b[38;5;28mcls\u001b[39m.from_json_file(config_file)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[33m\"\u001b[39m\u001b[33mpeft_type\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Can't find 'adapter_config.json' at '/home/mango/ERNIE/PaddleOCR-VL-SFT-test01'"
     ]
    }
   ],
   "source": [
    "# pip install -U transformers peft accelerate safetensors\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from peft import get_peft_model\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = \"PaddlePaddle/PaddleOCR-VL\"  # 학습 때 쓴 베이스와 동일\n",
    "lora_dir   = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\"\n",
    "hf_out_dir = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-hf-merged\"\n",
    "\n",
    "pre_model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(base_model, lora_dir)   # 어댑터 로드\n",
    "#model = model.merge_and_unload()                     # LoRA 병합\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)\n",
    "# model.save_pretrained(hf_out_dir, safe_serialization=True)\n",
    "# processor.save_pretrained(hf_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e549647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/meister1378/vl-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='meister1378/vl-lora')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=\"meister1378/vl-lora\", exist_ok=True)\n",
    "# api.upload_folder(\n",
    "#     repo_id=\"meister1378/vl-lora\",\n",
    "#     folder_path=\"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01/\",  # peft_model*.safetensors 등이 있는 폴더\n",
    "#     commit_message=\"upload lora adapter\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81769384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0487e0b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mmeister1378/ocr-vl-lora\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33mtest.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     20\u001b[39m     model_path, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m, torch_dtype=torch.bfloat16\n\u001b[32m     21\u001b[39m ).to(DEVICE).eval()\n\u001b[32m     22\u001b[39m processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/PIL/Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'test.png'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CHOSEN_TASK = \"ocr\"  # Options: 'ocr' | 'table' | 'chart' | 'formula'\n",
    "PROMPTS = {\n",
    "    \"ocr\": \"OCR:\",\n",
    "    \"table\": \"Table Recognition:\",\n",
    "    \"formula\": \"Formula Recognition:\",\n",
    "    \"chart\": \"Chart Recognition:\",\n",
    "}\n",
    "\n",
    "model_path = \"meister1378/ocr-vl-lora\"\n",
    "image_path = \"test.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True, torch_dtype=torch.bfloat16\n",
    ").to(DEVICE).eval()\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",         \n",
    "     \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": PROMPTS[CHOSEN_TASK]},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True, \t\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "outputs = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b873aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "meister1378/ocr-vl-lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
