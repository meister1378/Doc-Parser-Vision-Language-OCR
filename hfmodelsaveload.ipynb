{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa483277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF PEFT 변환 완료: /home/mango/ERNIE/PaddleOCR-VL-SFT-test02-hf\n"
     ]
    }
   ],
   "source": [
    "import os, json, shutil\n",
    "\n",
    "src = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test02\"             # ERNIE 출력\n",
    "dst = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test02-hf\"          # 변환 출력\n",
    "\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "# 1) peft_model-*.safetensors -> adapter_model-*.safetensors\n",
    "for fn in os.listdir(src):\n",
    "    if fn.startswith(\"peft_model\") and fn.endswith(\".safetensors\"):\n",
    "        shutil.copy2(os.path.join(src, fn),\n",
    "                     os.path.join(dst, fn.replace(\"peft_model\", \"adapter_model\")))\n",
    "\n",
    "# 2) 인덱스 파일도 이름 치환\n",
    "idx_in = os.path.join(src, \"peft_model.safetensors.index.json\")\n",
    "if os.path.exists(idx_in):\n",
    "    with open(idx_in, \"r\") as f: idx = json.load(f)\n",
    "    if \"weight_map\" in idx:\n",
    "        idx[\"weight_map\"] = {k: v.replace(\"peft_model\", \"adapter_model\")\n",
    "                             for k, v in idx[\"weight_map\"].items()}\n",
    "    with open(os.path.join(dst, \"adapter_model.safetensors.index.json\"), \"w\") as f:\n",
    "        json.dump(idx, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 3) lora_config.json -> adapter_config.json (필수 키만 매핑)\n",
    "with open(os.path.join(src, \"lora_config.json\"), \"r\") as f:\n",
    "    lc = json.load(f)\n",
    "\n",
    "adapter_cfg = {\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"r\": lc.get(\"lora_rank\", 16),\n",
    "    \"lora_alpha\": lc.get(\"lora_alpha\", 32),\n",
    "    \"lora_dropout\": lc.get(\"lora_dropout\", 0.05),\n",
    "    \"bias\": \"none\",\n",
    "    \"target_modules\": lc.get(\"target_modules\", []),\n",
    "    \"inference_mode\": True\n",
    "}\n",
    "with open(os.path.join(dst, \"adapter_config.json\"), \"w\") as f:\n",
    "    json.dump(adapter_cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"HF PEFT 변환 완료:\", dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a3063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e31c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1110 22:36:10.879169 22843 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-10 22:36:12,904] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:12,904] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:12,906] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,476] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,636] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,640] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,645] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,650] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,660] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,666] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,671] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,677] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,681] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,688] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,695] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,703] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,721] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,732] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,739] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,745] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,756] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:17,762] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,142] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,143] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,147] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,148] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-10 22:36:18,158] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,196] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,304] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,407] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:18,410] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/generation_config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA merge] merged=0, skipped=0, scale=1.0\n",
      "No pairs merged. Check LoRA keys sample: ['model.model.layers.0.self_attn.o_proj.lora_A', 'model.model.layers.0.self_attn.o_proj.lora_B', 'model.model.layers.0.mlp.down_proj.lora_A', 'model.model.layers.0.mlp.down_proj.lora_B', 'model.model.layers.1.self_attn.o_proj.lora_A', 'model.model.layers.1.self_attn.o_proj.lora_B', 'model.model.layers.1.mlp.down_proj.lora_A', 'model.model.layers.1.mlp.down_proj.lora_B', 'model.model.layers.2.self_attn.o_proj.lora_A', 'model.model.layers.2.self_attn.o_proj.lora_B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-10 22:36:21,876] [    INFO]\u001b[0m - Model weights saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:21,884] [    INFO]\u001b[0m - tokenizer config file saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:21,884] [    INFO]\u001b[0m - Special tokens file saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:21,885] [    INFO]\u001b[0m - added tokens file saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/added_tokens.json\u001b[0m\n",
      "\u001b[32m[2025-11-10 22:36:21,887] [    INFO]\u001b[0m - Image processor saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/preprocessor_config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged  total size (MB): 1832.11\n"
     ]
    }
   ],
   "source": [
    "# pip install paddlepaddle paddleformers\n",
    "import os, json, numpy as np\n",
    "import paddle\n",
    "from paddle import nn, tensor as T\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "BASE = \"PaddlePaddle/PaddleOCR-VL\"                  # 학습에 사용한 베이스와 동일\n",
    "LORA = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test01\"  # lora_config.json + peft_model-*.safetensors\n",
    "OUT  = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# 1) 베이스 로드 (HF→Paddle 변환 필요 시 convert_from_hf=True)\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(\n",
    "    BASE, convert_from_hf=True\n",
    ")\n",
    "tok   = Ernie_4_5_Tokenizer = Ernie4_5_Tokenizer.from_pretrained(\n",
    "    BASE, padding_side=\"right\", model_max_length=4096\n",
    ")\n",
    "proc  = SiglipImageProcessor.from_pretrained(BASE)\n",
    "\n",
    "# 2) LoRA 어댑터 적용(ERNIE 포맷에서 직접 로드)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=LORA)\n",
    "model.eval()\n",
    "\n",
    "# 3) LoRA 수동 병합: ΔW = (B @ A) * (alpha / r); base.W += ΔW\n",
    "with open(os.path.join(LORA, \"lora_config.json\"), \"r\") as f:\n",
    "    lcfg = json.load(f)\n",
    "alpha = float(lcfg.get(\"lora_alpha\", 16))\n",
    "r     = float(lcfg.get(\"lora_rank\", 16))\n",
    "scale = alpha / max(r, 1.0)\n",
    "\n",
    "state  = model.state_dict()\n",
    "params = dict(model.named_parameters())\n",
    "\n",
    "def _to_np(x: T.Variable): return np.array(x)\n",
    "\n",
    "def add_inplace(param: nn.Parameter, delta_np: np.ndarray):\n",
    "    base_np = _to_np(param)\n",
    "    param.set_value(T.to_tensor(base_np + delta_np, dtype=param.dtype))\n",
    "\n",
    "merged_cnt, skipped = 0, 0\n",
    "\n",
    "def try_merge_pair(a_key, b_key):\n",
    "    global merged_cnt, skipped\n",
    "    if a_key not in state or b_key not in state:\n",
    "        skipped += 1; return\n",
    "    if a_key.endswith(\".lora_A.weight\"):\n",
    "        base_key = a_key[:-len(\".lora_A.weight\")] + \".weight\"\n",
    "    elif a_key.endswith(\".lora_down.weight\"):\n",
    "        base_key = a_key[:-len(\".lora_down.weight\")] + \".weight\"\n",
    "    else:\n",
    "        skipped += 1; return\n",
    "    if base_key not in params:\n",
    "        skipped += 1; return\n",
    "    A = state[a_key]   # [r, in]\n",
    "    B = state[b_key]   # [out, r]\n",
    "    try:\n",
    "        delta = T.matmul(B, A) * scale\n",
    "    except Exception:\n",
    "        delta = T.to_tensor(_to_np(B) @ _to_np(A), dtype=B.dtype) * scale\n",
    "    add_inplace(params[base_key], _to_np(delta))\n",
    "    merged_cnt += 1\n",
    "\n",
    "# 패턴1: lora_A/lora_B\n",
    "for k in list(state.keys()):\n",
    "    if k.endswith(\".lora_A.weight\"):\n",
    "        try_merge_pair(k, k.replace(\".lora_A.weight\", \".lora_B.weight\"))\n",
    "\n",
    "# 패턴2: lora_down/lora_up\n",
    "for k in list(state.keys()):\n",
    "    if k.endswith(\".lora_down.weight\"):\n",
    "        try_merge_pair(k, k.replace(\".lora_down.weight\", \".lora_up.weight\"))\n",
    "\n",
    "print(f\"[LoRA merge] merged={merged_cnt}, skipped={skipped}, scale={scale}\")\n",
    "\n",
    "# (선택) 어댑터 파라미터 0으로 막기(중복 적용 방지)\n",
    "for name, p in params.items():\n",
    "    if name.endswith((\"lora_A.weight\",\"lora_B.weight\",\"lora_down.weight\",\"lora_up.weight\")):\n",
    "        p.set_value(T.zeros_like(p))\n",
    "\n",
    "# 4) 병합된 베이스 가중치 저장 (용량이 커져야 정상)\n",
    "model.save_pretrained(OUT)\n",
    "tok.save_pretrained(OUT)\n",
    "proc.save_pretrained(OUT)\n",
    "print(\"병합 완료:\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e92ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-11 09:48:31,797] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:31,799] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:31,801] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,200] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,370] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,373] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,381] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,387] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,392] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,399] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,405] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,409] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,414] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,422] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,428] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,432] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,440] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,452] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,465] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,477] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,486] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:35,504] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,331] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,332] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,335] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,335] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,385] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,518] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,635] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,644] [    INFO]\u001b[0m - tokenizer config file saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,645] [    INFO]\u001b[0m - Special tokens file saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,646] [    INFO]\u001b[0m - added tokens file saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/added_tokens.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 09:48:36,648] [    INFO]\u001b[0m - Image processor saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/preprocessor_config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddle(ERNIE) 저장 완료: /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged\n"
     ]
    }
   ],
   "source": [
    "# ERNIE(Paddle) 방식: 베이스 로드 → LoRA 어댑터(체크포인트) 적용 → 저장\n",
    "# pip install paddlepaddle paddlefsl paddleformers Pillow\n",
    "\n",
    "import os\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "# 1) 경로 설정\n",
    "BASE_MODEL_ID = \"PaddlePaddle/PaddleOCR-VL\"  # 학습 시 사용한 베이스와 동일해야 함\n",
    "LORA_ROOT_DIR = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test04\"   # peft_model-*.safetensors, lora_config.json 이 있는 루트\n",
    "SAVE_DIR      = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 2) 베이스 모델/토크나이저/이미지프로세서 로드 (HF→Paddle 변환 필요시 convert_from_hf=True)\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    convert_from_hf=True,  # HF 허브 베이스를 Paddle로 변환 로드\n",
    ")\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(\n",
    "    BASE_MODEL_ID, padding_side=\"right\", model_max_length=4096\n",
    ")\n",
    "image_proc = SiglipImageProcessor.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "# 3) LoRA 어댑터 적용(체크포인트 루트에서 직접 로드)\n",
    "#   ERNIE 포맷: LORA_ROOT_DIR 안에 lora_config.json + peft_model-*.safetensors 가 있어야 합니다.\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=LORA_ROOT_DIR)\n",
    "model.eval()\n",
    "\n",
    "# (선택) 병합 API가 있을 경우 병합 저장\n",
    "if hasattr(model, \"merge_and_unload\"):\n",
    "    try:\n",
    "        model = model.merge_and_unload()\n",
    "        print(\"LoRA merged into base weights.\")\n",
    "    except Exception as e:\n",
    "        print(f\"merge_and_unload 미지원/실패: {e}; 어댑터 래핑 상태로 저장합니다.\")\n",
    "\n",
    "# 4) 저장(ERNIE/Paddle 포맷)\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "image_proc.save_pretrained(SAVE_DIR)\n",
    "print(\"Paddle(ERNIE) 저장 완료:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c4adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1111 10:18:47.659309 17906 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-11 10:18:49,703] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:49,704] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:49,705] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,727] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,866] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,869] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,873] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,880] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,884] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,889] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,894] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,899] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,904] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,908] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,915] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,919] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,922] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,930] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,934] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,938] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,944] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:53,949] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:54,312] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:54,313] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:54,315] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:54,315] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-11 10:18:54,327] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:54,368] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-11 10:18:54,482] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[35m[2025-11-11 10:18:54,597] [   DEBUG]\u001b[0m - Frozen parameters: 9.59e+08 || Trainable parameters:1.03e+06 || Total parameters:9.60e+08|| Trainable:0.11%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "# 1) 경로 설정\n",
    "base_model = \"PaddlePaddle/PaddleOCR-VL\"  # 베이스 모델 허브 경로(또는 로컬 경로)\n",
    "lora_dir   = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test04/checkpoint-20/\"  # peft_model*.safetensors 있는 디렉터리\n",
    "\n",
    "# 2) 베이스 모델/토크나이저/이미지 프로세서 로드\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(base_model, convert_from_hf=True)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(base_model, padding_side=\"right\", model_max_length=8192)\n",
    "image_preprocessor = SiglipImageProcessor.from_pretrained(base_model)\n",
    "\n",
    "# 3) LoRA 어댑터 적용 (출력 디렉터리에서)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=lora_dir)\n",
    "model.eval()    \n",
    "model.mark_only_lora_as_trainable()\n",
    "model.print_trainable_parameters()\n",
    "# (선택) LoRA 병합 후 고정이 필요하면, 다음 API가 지원되면 사용:\n",
    "# model.merge_and_unload()  # 지원되는 경우에만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db10a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# CHOSEN_TASK = \"ocr\"  # Options: 'ocr' | 'table' | 'chart' | 'formula'\n",
    "# PROMPTS = {\n",
    "#     \"ocr\": \"OCR:\",\n",
    "#     \"table\": \"Table Recognition:\",\n",
    "#     \"formula\": \"Formula Recognition:\",\n",
    "#     \"chart\": \"Chart Recognition:\",\n",
    "# }\n",
    "\n",
    "# image_path = \"000000014_경남.jpg\"\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(base_model)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\",         \n",
    "#      \"content\": [\n",
    "#             {\"type\": \"image\", \"image\": image},\n",
    "#             {\"type\": \"text\", \"text\": PROMPTS[CHOSEN_TASK]},\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "# inputs = processor.apply_chat_template(\n",
    "#     messages, \n",
    "#     tokenize=True, \n",
    "#     add_generation_prompt=True, \t\n",
    "#     return_dict=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# outputs = model.infer(**inputs, max_new_tokens=1024)\n",
    "# outputs = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1703f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1111 13:29:50.981730 23682 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-11 13:29:53,141] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:53,143] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:53,145] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,286] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,449] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,453] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,459] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,464] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,468] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,473] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,479] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,484] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,487] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,493] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,498] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,504] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,509] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,515] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,519] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,523] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,530] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,535] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,908] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,910] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,912] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,913] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-11 13:29:57,926] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:57,969] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:29:58,085] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "BASE = \"PaddlePaddle/PaddleOCR-VL\"                     # 학습 베이스와 동일\n",
    "LORA = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test02\"     # LoRA 출력 디렉터리(ERNIE 포맷)\n",
    "\n",
    "# 1) 베이스 로드(HF->Paddle 변환 필요 시)\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE, convert_from_hf=True)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(LORA, padding_side=\"right\", model_max_length=4096)\n",
    "image_preprocess = SiglipImageProcessor.from_pretrained(LORA)\n",
    "\n",
    "# 2) LoRA 어댑터 적용\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=LORA)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c2c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[33m[2025-11-11 14:23:12,202] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlex.inference.models.doc_vlm.processors.paddleocr_vl._paddleocr_vl import PaddleOCRVLProcessor\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer  # paddlex 배포 토크나이저가 HF 포맷\n",
    "\n",
    "model_dir = \"./PaddleOCR-VL-SFT-test03\"  # --vl_rec_model_dir와 동일한 구조\n",
    "image_processor = SiglipImageProcessor.from_pretrained(model_dir)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "processor = PaddleOCRVLProcessor(image_processor=image_processor, tokenizer=tokenizer)\n",
    "# 이후 processor.preprocess(...) / processor.postprocess(...) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65fe194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlex.inference.models.doc_vlm.processors.paddleocr_vl._paddleocr_vl.PaddleOCRVLProcessor at 0x7d58adedd640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d0fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CHOSEN_TASK = \"ocr\"  # Options: 'ocr' | 'table' | 'chart' | 'formula'\n",
    "PROMPTS = {\n",
    "    \"ocr\": \"OCR:\",\n",
    "    \"table\": \"Table Recognition:\",\n",
    "    \"formula\": \"Formula Recognition:\",\n",
    "    \"chart\": \"Chart Recognition:\",\n",
    "}\n",
    "\n",
    "model_path = \"PaddlePaddle/PaddleOCR-VL\"\n",
    "image_path = \"000000014_경남.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True, torch_dtype=torch.bfloat16\n",
    ").to(DEVICE).eval()\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",         \n",
    "     \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": PROMPTS[CHOSEN_TASK]},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True, \t\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcabc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[100273,   2969,  93963,  93919, 101305, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295, 100295,\n",
       "         100295, 100295, 100295, 100295, 100295, 100295, 100295, 101306,  93972,\n",
       "           2497,  93963,     23,  92267,  93963,  93919]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0'), 'pixel_values': tensor([[[[0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          ...,\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9922, 0.9922]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.9922, 0.9922],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.9922, 0.9922]],\n",
       "\n",
       "         [[0.9765, 0.9765, 0.9765,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9765, 0.9765, 0.9765,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9765, 0.9765, 0.9765,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          ...,\n",
       "          [0.9765, 0.9765, 0.9765,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9765, 0.9765, 0.9765,  ..., 0.9843, 0.9843, 0.9922],\n",
       "          [0.9765, 0.9765, 0.9765,  ..., 0.9843, 0.9843, 0.9922]]],\n",
       "\n",
       "\n",
       "        [[[0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9765, 0.9765],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9765, 0.9765],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9765, 0.9765],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9765, 0.9765],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9765, 0.9765],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9765, 0.9765]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9608, 0.9608],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9686, 0.9608, 0.9608],\n",
       "          [0.9843, 0.9765, 0.9765,  ..., 0.9686, 0.9686, 0.9608],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9686, 0.9608, 0.9608],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9686, 0.9608, 0.9608],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9686, 0.9608, 0.9608]],\n",
       "\n",
       "         [[0.9843, 0.9922, 0.9922,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9765, 0.9843, 0.9922,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [0.9765, 0.9765, 0.9843,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9922, 0.9922],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9922, 0.9922]],\n",
       "\n",
       "         [[0.9294, 0.9216, 0.9294,  ..., 0.9373, 0.9373, 0.9451],\n",
       "          [0.9216, 0.9216, 0.9216,  ..., 0.9373, 0.9373, 0.9451],\n",
       "          [0.9216, 0.9137, 0.9137,  ..., 0.9451, 0.9451, 0.9451],\n",
       "          ...,\n",
       "          [0.9451, 0.9373, 0.9373,  ..., 0.9373, 0.9373, 0.9451],\n",
       "          [0.9451, 0.9373, 0.9373,  ..., 0.9373, 0.9373, 0.9451],\n",
       "          [0.9451, 0.9373, 0.9373,  ..., 0.9373, 0.9373, 0.9451]]],\n",
       "\n",
       "\n",
       "        [[[0.9529, 0.9529, 0.9451,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9529, 0.9529, 0.9451,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9529, 0.9529, 0.9451,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          ...,\n",
       "          [0.9529, 0.9529, 0.9451,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9529, 0.9529, 0.9451,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9529, 0.9529, 0.9451,  ..., 0.9216, 0.9216, 0.9216]],\n",
       "\n",
       "         [[0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          ...,\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922]],\n",
       "\n",
       "         [[0.9451, 0.9451, 0.9529,  ..., 0.9843, 0.9843, 0.9843],\n",
       "          [0.9451, 0.9451, 0.9529,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9451, 0.9451, 0.9529,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          ...,\n",
       "          [0.9451, 0.9451, 0.9529,  ..., 0.9686, 0.9686, 0.9686],\n",
       "          [0.9451, 0.9451, 0.9529,  ..., 0.9686, 0.9686, 0.9686],\n",
       "          [0.9451, 0.9451, 0.9529,  ..., 0.9686, 0.9686, 0.9686]]],\n",
       "\n",
       "\n",
       "        [[[0.9216, 0.9294, 0.9373,  ..., 0.9451, 0.9451, 0.9451],\n",
       "          [0.9216, 0.9294, 0.9373,  ..., 0.9451, 0.9451, 0.9451],\n",
       "          [0.9216, 0.9294, 0.9373,  ..., 0.9451, 0.9451, 0.9451],\n",
       "          ...,\n",
       "          [0.9216, 0.9294, 0.9373,  ..., 0.9451, 0.9451, 0.9451],\n",
       "          [0.9216, 0.9294, 0.9373,  ..., 0.9451, 0.9451, 0.9451],\n",
       "          [0.9216, 0.9294, 0.9373,  ..., 0.9451, 0.9451, 0.9451]],\n",
       "\n",
       "         [[0.9922, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9922, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.9843, 0.9922, 0.9922,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9843, 0.9843, 0.9922,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9765, 0.9843, 0.9843,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          ...,\n",
       "          [0.9765, 0.9765, 0.9843,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9765, 0.9765, 0.9843,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9765, 0.9765, 0.9843,  ..., 0.9765, 0.9765, 0.9765]]]],\n",
       "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 20, 40]], device='cuda:0')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce83a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-25 16:08:25,515] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:25,518] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:25,522] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,296] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,409] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,413] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,417] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,422] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,425] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,428] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,431] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,435] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,437] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,441] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,444] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,446] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,448] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,455] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,459] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,463] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,466] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:33,471] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:34,604] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:34,604] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:34,607] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:34,608] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:34,673] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:35,755] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:35,958] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:35,960] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 16:08:41,852] [    INFO]\u001b[0m - Model weights saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model.safetensors\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "from paddleformers.peft import LoRAConfig, LoRAModel, PrefixConfig, PrefixModelForCausalLM\n",
    "from paddleformers.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from paddleformers.peft import LoRAAutoModel\n",
    "BASE = \"PaddlePaddle/PaddleOCR-VL\"                     # 학습 베이스와 동일\n",
    "LORA = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test06\"     # LoRA 출력 디렉터리(ERNIE 포맷)\n",
    "\n",
    "# 1) 베이스 로드(HF->Paddle 변환 필요 시)\n",
    "base_model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE, convert_from_hf=True)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(LORA, padding_side=\"right\", model_max_length=4096)\n",
    "image_preprocess = SiglipImageProcessor.from_pretrained(LORA)\n",
    "\n",
    "#lora_config = LoRAConfig.from_pretrained(LORA)\n",
    "#dtype = lora_config.dtype\n",
    "#lora_config.merge_weights = True\n",
    "# 2) LoRA 어댑터 적용\n",
    "\n",
    "lora_model = LoRAModel.from_pretrained(base_model, lora_path=LORA)\n",
    "lora_model.merge()\n",
    "lora_model.restore_original_model()\n",
    "if hasattr(lora_model, \"model\"):\n",
    "    # LoRAModel이 감싸고 있는 실제 모델 객체 접근\n",
    "    merged_base_model = lora_model.model\n",
    "    #print(merged_base_model)\n",
    "    merged_base_model.save_pretrained(\n",
    "    LORA,\n",
    "    safe_serialization=True,   # HF safetensors로 저장\n",
    ")\n",
    "#lora_model.model.save_pretrained(LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728b9cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 파일 로드 중: /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model.safetensors\n",
      "🔧 Transposing: lm_head.weight torch.Size([1024, 103424]) -> torch.Size([103424, 1024])\n",
      "🔧 Transposing: mlp_AR.linear_1.weight torch.Size([4608, 4608]) -> torch.Size([4608, 4608])\n",
      "🔧 Transposing: mlp_AR.linear_2.weight torch.Size([4608, 1024]) -> torch.Size([1024, 4608])\n",
      "🔧 Transposing: model.layers.0.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.0.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.0.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.0.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.0.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.0.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.0.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.1.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.1.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.1.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.1.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.1.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.1.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.1.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.10.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.10.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.10.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.10.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.10.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.10.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.10.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.11.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.11.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.11.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.11.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.11.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.11.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.11.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.12.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.12.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.12.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.12.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.12.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.12.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.12.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.13.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.13.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.13.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.13.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.13.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.13.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.13.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.14.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.14.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.14.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.14.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.14.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.14.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.14.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.15.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.15.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.15.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.15.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.15.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.15.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.15.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.16.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.16.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.16.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.16.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.16.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.16.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.16.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.17.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.17.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.17.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.17.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.17.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.17.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.17.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.2.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.2.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.2.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.2.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.2.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.2.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.2.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.3.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.3.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.3.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.3.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.3.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.3.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.3.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.4.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.4.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.4.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.4.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.4.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.4.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.4.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.5.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.5.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.5.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.5.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.5.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.5.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.5.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.6.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.6.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.6.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.6.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.6.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.6.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.6.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.7.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.7.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.7.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.7.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.7.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.7.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.7.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.8.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.8.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.8.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.8.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.8.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.8.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.8.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.9.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.9.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.9.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.9.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.9.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.9.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.9.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.head.attention.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.head.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.head.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "\n",
      "💾 총 294개의 텐서를 뒤집어 저장합니다...\n",
      "✅ 저장 완료: /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model_fixed.safetensors\n",
      "👉 이 파일의 이름을 'model.safetensors'로 변경해서 사용하세요!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    from safetensors import safe_open\n",
    "    from safetensors.torch import save_file\n",
    "    import torch \n",
    "except ImportError:\n",
    "    print(\"❌ 'pip install safetensors torch' 필요\")\n",
    "    exit()\n",
    "\n",
    "# =========================================================\n",
    "# [설정] 문제가 있는 LoRA 모델 경로\n",
    "# =========================================================\n",
    "TARGET_FILE = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model.safetensors\"\n",
    "# =========================================================\n",
    "\n",
    "def fix_transposed_weights():\n",
    "    print(f\"📂 파일 로드 중: {TARGET_FILE}\")\n",
    "    \n",
    "    if not os.path.exists(TARGET_FILE):\n",
    "        print(\"❌ 파일 없음\")\n",
    "        return\n",
    "\n",
    "    tensors_to_save = {}\n",
    "    fixed_count = 0\n",
    "    \n",
    "    with safe_open(TARGET_FILE, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for key in f.keys():\n",
    "            tensor = f.get_tensor(key)\n",
    "            \n",
    "            # 2D 텐서(Linear Weight)인 경우 전치 대상인지 확인\n",
    "            if len(tensor.shape) == 2:\n",
    "                # Transpose 대상 키워드 필터링\n",
    "                if \"proj.weight\" in key or \"fc1.weight\" in key or \"fc2.weight\" in key or \"lm_head.weight\" in key or \"linear\" in key:\n",
    "                    print(f\"🔧 Transposing: {key} {tensor.shape} -> {tensor.t().shape}\")\n",
    "                    \n",
    "                    # [핵심 수정] .t() 뒤에 .contiguous() 추가\n",
    "                    tensor = tensor.t().contiguous() \n",
    "                    \n",
    "                    fixed_count += 1\n",
    "            \n",
    "            tensors_to_save[key] = tensor\n",
    "\n",
    "    # 3. 저장\n",
    "    if fixed_count > 0:\n",
    "        print(f\"\\n💾 총 {fixed_count}개의 텐서를 뒤집어 저장합니다...\")\n",
    "        output_path = TARGET_FILE.replace(\".safetensors\", \"_fixed.safetensors\")\n",
    "        \n",
    "        # 메타데이터 없이 저장 (순수 텐서 데이터)\n",
    "        save_file(tensors_to_save, output_path)\n",
    "        \n",
    "        print(f\"✅ 저장 완료: {output_path}\")\n",
    "        print(\"👉 이 파일의 이름을 'model.safetensors'로 변경해서 사용하세요!\")\n",
    "    else:\n",
    "        print(\"⚠️ 수정할 텐서를 찾지 못했습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fix_transposed_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315799aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 파일 로드 중: /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model.safetensors\n",
      "🔧 Transposing: lm_head.weight torch.Size([1024, 103424]) -> torch.Size([103424, 1024])\n",
      "🔧 Transposing: mlp_AR.linear_1.weight torch.Size([4608, 4608]) -> torch.Size([4608, 4608])\n",
      "🔧 Transposing: mlp_AR.linear_2.weight torch.Size([4608, 1024]) -> torch.Size([1024, 4608])\n",
      "🔧 Transposing: model.layers.0.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.0.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.0.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.0.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.0.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.0.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.0.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.1.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.1.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.1.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.1.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.1.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.1.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.1.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.10.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.10.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.10.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.10.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.10.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.10.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.10.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.11.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.11.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.11.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.11.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.11.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.11.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.11.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.12.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.12.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.12.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.12.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.12.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.12.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.12.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.13.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.13.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.13.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.13.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.13.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.13.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.13.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.14.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.14.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.14.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.14.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.14.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.14.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.14.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.15.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.15.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.15.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.15.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.15.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.15.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.15.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.16.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.16.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.16.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.16.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.16.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.16.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.16.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.17.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.17.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.17.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.17.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.17.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.17.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.17.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.2.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.2.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.2.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.2.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.2.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.2.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.2.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.3.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.3.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.3.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.3.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.3.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.3.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.3.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.4.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.4.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.4.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.4.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.4.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.4.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.4.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.5.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.5.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.5.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.5.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.5.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.5.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.5.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.6.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.6.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.6.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.6.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.6.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.6.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.6.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.7.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.7.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.7.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.7.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.7.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.7.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.7.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.8.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.8.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.8.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.8.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.8.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.8.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.8.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.9.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "🔧 Transposing: model.layers.9.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.9.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "🔧 Transposing: model.layers.9.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: model.layers.9.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "🔧 Transposing: model.layers.9.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "🔧 Transposing: model.layers.9.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.12.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.13.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.14.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.15.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.16.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.17.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.18.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.19.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.20.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.21.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.22.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.23.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.24.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.25.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.26.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.head.attention.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "🔧 Transposing: visual.vision_model.head.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "🔧 Transposing: visual.vision_model.head.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "\n",
      "💾 총 294개의 텐서를 뒤집어 저장합니다...\n",
      "✅ 저장 완료: /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model_fixed.safetensors\n",
      "👉 이 파일의 이름을 'model.safetensors'로 변경해서 사용하세요!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    from safetensors import safe_open\n",
    "    from safetensors.torch import save_file\n",
    "    import torch \n",
    "except ImportError:\n",
    "    print(\"❌ 'pip install safetensors torch' 필요\")\n",
    "    exit()\n",
    "\n",
    "# =========================================================\n",
    "# [설정] 문제가 있는 LoRA 모델 경로\n",
    "# =========================================================\n",
    "TARGET_FILE = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model.safetensors\"\n",
    "# =========================================================\n",
    "\n",
    "def fix_transposed_weights():\n",
    "    print(f\"📂 파일 로드 중: {TARGET_FILE}\")\n",
    "    \n",
    "    if not os.path.exists(TARGET_FILE):\n",
    "        print(\"❌ 파일 없음\")\n",
    "        return\n",
    "\n",
    "    tensors_to_save = {}\n",
    "    fixed_count = 0\n",
    "    \n",
    "    with safe_open(TARGET_FILE, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for key in f.keys():\n",
    "            tensor = f.get_tensor(key)\n",
    "            \n",
    "            # 2D 텐서(Linear Weight)인 경우 전치 대상인지 확인\n",
    "            if len(tensor.shape) == 2:\n",
    "                # Transpose 대상 키워드 필터링\n",
    "                if \"proj.weight\" in key or \"fc1.weight\" in key or \"fc2.weight\" in key or \"lm_head.weight\" in key or \"linear\" in key:\n",
    "                    print(f\"🔧 Transposing: {key} {tensor.shape} -> {tensor.t().shape}\")\n",
    "                    \n",
    "                    # [핵심 수정] .t() 뒤에 .contiguous() 추가\n",
    "                    tensor = tensor.t().contiguous() \n",
    "                    \n",
    "                    fixed_count += 1\n",
    "            \n",
    "            tensors_to_save[key] = tensor\n",
    "\n",
    "    # 3. 저장\n",
    "    if fixed_count > 0:\n",
    "        print(f\"\\n💾 총 {fixed_count}개의 텐서를 뒤집어 저장합니다...\")\n",
    "        output_path = TARGET_FILE.replace(\".safetensors\", \"_fixed.safetensors\")\n",
    "        \n",
    "        # 메타데이터 없이 저장 (순수 텐서 데이터)\n",
    "        save_file(tensors_to_save, output_path)\n",
    "        \n",
    "        print(f\"✅ 저장 완료: {output_path}\")\n",
    "        print(\"👉 이 파일의 이름을 'model.safetensors'로 변경해서 사용하세요!\")\n",
    "    else:\n",
    "        print(\"⚠️ 수정할 텐서를 찾지 못했습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fix_transposed_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba76f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Safetensors 메타데이터 추출 중...\n",
      "\n",
      "📊 통계:\n",
      "   - Base Keys: 620\n",
      "   - LoRA Keys: 620\n",
      "\n",
      "============================================================\n",
      "🚨 [1] 누락된 키 (Base O, LoRA X) - 총 0개\n",
      "============================================================\n",
      "   ✅ 없음 (Good)\n",
      "\n",
      "============================================================\n",
      "🚨 [2] 불필요한 키 (Base X, LoRA O) - 총 0개\n",
      "============================================================\n",
      "   ✅ 없음 (Good)\n",
      "\n",
      "============================================================\n",
      "📏 [3] Shape 불일치\n",
      "============================================================\n",
      "   ✅ Shape 모두 일치 (Good)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    from safetensors import safe_open\n",
    "except ImportError:\n",
    "    print(\"❌ 'safetensors' 라이브러리가 필요합니다. (pip install safetensors)\")\n",
    "    exit()\n",
    "\n",
    "# =========================================================\n",
    "# [설정] 정확한 파일 경로 (safetensors 파일까지 포함)\n",
    "# =========================================================\n",
    "# 예: /home/user/.cache/huggingface/.../model.safetensors\n",
    "BASE_FILE = \"/home/mango/ERNIE/PaddlePaddle/PaddleOCR-VL/model.safetensors\" \n",
    "\n",
    "# 예: /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model.safetensors\n",
    "LORA_FILE = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model_fixed.safetensors\"\n",
    "# =========================================================\n",
    "\n",
    "def load_safetensors_meta(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"❌ 파일 없음: {path}\")\n",
    "        return None\n",
    "    \n",
    "    meta = {}\n",
    "    try:\n",
    "        with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                # 텐서 자체를 로드하지 않고 shape만 가져오고 싶지만,\n",
    "                # safe_open은 get_tensor() 해야 shape을 알 수 있는 경우가 많음.\n",
    "                # (메타데이터만 빠르게 보려면 f.get_slice(key).get_shape() 사용)\n",
    "                tensor_slice = f.get_slice(key)\n",
    "                meta[key] = tensor_slice.get_shape()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 읽기 오류 ({path}): {e}\")\n",
    "        return None\n",
    "        \n",
    "    return meta\n",
    "\n",
    "def compare_safetensors():\n",
    "    print(\"🔍 Safetensors 메타데이터 추출 중...\")\n",
    "    \n",
    "    base_meta = load_safetensors_meta(BASE_FILE)\n",
    "    lora_meta = load_safetensors_meta(LORA_FILE)\n",
    "    \n",
    "    if not base_meta or not lora_meta:\n",
    "        return\n",
    "\n",
    "    base_keys = set(base_meta.keys())\n",
    "    lora_keys = set(lora_meta.keys())\n",
    "\n",
    "    print(f\"\\n📊 통계:\")\n",
    "    print(f\"   - Base Keys: {len(base_keys)}\")\n",
    "    print(f\"   - LoRA Keys: {len(lora_keys)}\")\n",
    "\n",
    "    # 1. 누락 (Missing)\n",
    "    missing = base_keys - lora_keys\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"🚨 [1] 누락된 키 (Base O, LoRA X) - 총 {len(missing)}개\")\n",
    "    print(\"=\"*60)\n",
    "    if missing:\n",
    "        for k in sorted(list(missing)):\n",
    "            print(f\"   ➖ {k}\")\n",
    "    else:\n",
    "        print(\"   ✅ 없음 (Good)\")\n",
    "\n",
    "    # 2. 불필요 (Extra)\n",
    "    extra = lora_keys - base_keys\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"🚨 [2] 불필요한 키 (Base X, LoRA O) - 총 {len(extra)}개\")\n",
    "    print(\"=\"*60)\n",
    "    if extra:\n",
    "        for k in sorted(list(extra)):\n",
    "            print(f\"   ➕ {k}\")\n",
    "    else:\n",
    "        print(\"   ✅ 없음 (Good)\")\n",
    "\n",
    "    # 3. Shape 불일치\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📏 [3] Shape 불일치\")\n",
    "    print(\"=\"*60)\n",
    "    common = base_keys & lora_keys\n",
    "    mismatch_cnt = 0\n",
    "    for k in sorted(list(common)):\n",
    "        if base_meta[k] != lora_meta[k]:\n",
    "            print(f\"   ⚠️ {k}\")\n",
    "            print(f\"       Base: {base_meta[k]}  vs  LoRA: {lora_meta[k]}\")\n",
    "            mismatch_cnt += 1\n",
    "            \n",
    "    if mismatch_cnt == 0:\n",
    "        print(\"   ✅ Shape 모두 일치 (Good)\")\n",
    "    else:\n",
    "        print(f\"   ❌ 총 {mismatch_cnt}개 Shape 불일치 발견\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_safetensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095722e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-25 14:52:52,329] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:52,330] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:52,332] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Base 모델 로드 중: PaddlePaddle/PaddleOCR-VL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-25 14:52:55,754] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,804] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,806] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,808] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,813] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,815] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,817] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,820] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,822] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,824] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,827] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,831] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,833] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,835] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,837] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,840] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,842] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,857] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:55,867] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:56,319] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:56,320] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:56,322] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:52:56,323] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로드 완료. (620 keys)\n",
      "👀 visual.vision_model.head.attention.in_proj_bias              | Shape: [3456]          | Dtype: paddle.bfloat16\n",
      "👀 visual.vision_model.head.attention.in_proj_weight            | Shape: [3456, 1152]    | Dtype: paddle.bfloat16\n",
      "👀 visual.vision_model.head.attention.out_proj.bias             | Shape: [1152]          | Dtype: paddle.bfloat16\n",
      "👀 visual.vision_model.head.attention.out_proj.weight           | Shape: [1152, 1152]    | Dtype: paddle.bfloat16\n",
      "\n",
      "✅ Base 모델 구조가 'base_model_dump.txt' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 설정\n",
    "BASE_MODEL_PATH = \"PaddlePaddle/PaddleOCR-VL\"   # Base 모델 폴더명 (또는 HuggingFace Hub ID)\n",
    "OUTPUT_TXT = \"base_model_dump.txt\"\n",
    "\n",
    "def dump_base_structure():\n",
    "    print(f\"🚀 Base 모델 로드 중: {BASE_MODEL_PATH}\")\n",
    "    try:\n",
    "        model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE_MODEL_PATH, convert_from_hf=True)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로드 실패: {e}\")\n",
    "        return\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    print(f\"✅ 로드 완료. ({len(state_dict)} keys)\")\n",
    "    \n",
    "    with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"=== Base Model Dump ({len(state_dict)} keys) ===\\n\")\n",
    "        f.write(f\"Source: {BASE_MODEL_PATH}\\n\\n\")\n",
    "        \n",
    "        sorted_keys = sorted(state_dict.keys())\n",
    "        \n",
    "        for key in sorted_keys:\n",
    "            tensor = state_dict[key]\n",
    "            # Shape과 Dtype 기록\n",
    "            info = f\"{key:<60} | Shape: {str(tensor.shape):<15} | Dtype: {tensor.dtype}\\n\"\n",
    "            f.write(info)\n",
    "            \n",
    "            # 비교를 위해 head 부분은 콘솔에도 출력\n",
    "            if \"head\" in key and \"attention\" in key:\n",
    "                print(f\"👀 {info.strip()}\")\n",
    "                \n",
    "    print(f\"\\n✅ Base 모델 구조가 '{OUTPUT_TXT}' 파일에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dump_base_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d71f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-25 14:25:13,215] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:13,216] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:13,217] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 [1] Base Model (원본) 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-25 14:25:16,562] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,685] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,688] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,692] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,698] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,702] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,707] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,712] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,719] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,723] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,726] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,734] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,739] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,743] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,749] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,754] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,758] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,785] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:16,789] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,285] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,286] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,288] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,289] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,294] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,297] [    INFO]\u001b[0m - Loading configuration file /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:17,299] [    INFO]\u001b[0m - Loading weights file /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/model_state.pdparams\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== [linear] 레이어 구조 분석 ====================\n",
      "📍 레이어 이름: mlp_AR.linear_1\n",
      "🧩 레이어 타입: <class 'paddle.nn.layer.common.Linear'>\n",
      "🔑 포함된 파라미터(Keys):\n",
      "   - weight                    | Shape: [4608, 4608]\n",
      "   - bias                      | Shape: [4608]\n",
      "\n",
      "\n",
      "🟠 [2] LoRA Model (래핑 후) 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-11-25 14:25:18,284] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,402] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,405] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,409] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,414] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,419] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,423] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,427] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,434] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,438] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,442] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,448] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,452] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,456] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,460] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,467] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,471] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,474] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,479] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,893] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,894] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at /home/mango/ERNIE/PaddleOCR-VL-SFT-test06.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,899] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 14:25:18,900] [    INFO]\u001b[0m - Loading configuration file /home/mango/ERNIE/PaddleOCR-VL-SFT-test06/generation_config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== [linear] 레이어 구조 분석 ====================\n",
      "📍 레이어 이름: mlp_AR.linear_1\n",
      "🧩 레이어 타입: <class 'paddle.nn.layer.common.Linear'>\n",
      "🔑 포함된 파라미터(Keys):\n",
      "   - weight                    | Shape: [4608, 4608]\n",
      "   - bias                      | Shape: [4608]\n"
     ]
    }
   ],
   "source": [
    "# 경로 설정\n",
    "BASE_MODEL_PATH = BASE\n",
    "LORA_PATH = LORA  # 실제 LoRA 경로로 수정하세요\n",
    "\n",
    "def print_layer_info(model, layer_name_keyword=\"q_proj\"):\n",
    "    \"\"\"\n",
    "    모델 내에서 특정 키워드가 포함된 첫 번째 레이어를 찾아 구조를 출력합니다.\n",
    "    보통 LoRA는 Attention의 q_proj, v_proj 등에 적용되므로 이를 확인합니다.\n",
    "    \"\"\"\n",
    "    found = False\n",
    "    print(f\"\\n{'='*20} [{layer_name_keyword}] 레이어 구조 분석 {'='*20}\")\n",
    "    \n",
    "    for name, sublayer in model.named_sublayers():\n",
    "        if layer_name_keyword in name:\n",
    "            print(f\"📍 레이어 이름: {name}\")\n",
    "            print(f\"🧩 레이어 타입: {type(sublayer)}\")\n",
    "            print(f\"🔑 포함된 파라미터(Keys):\")\n",
    "            for param_name, param in sublayer.named_parameters():\n",
    "                print(f\"   - {param_name:<25} | Shape: {param.shape}\")\n",
    "            found = True\n",
    "            break\n",
    "            \n",
    "    if not found:\n",
    "        print(f\"'{layer_name_keyword}'가 포함된 레이어를 찾지 못했습니다.\")\n",
    "\n",
    "def compare_structures():\n",
    "    # 1. Base Model 로드 및 구조 확인\n",
    "    print(\"\\n🔵 [1] Base Model (원본) 로드 중...\")\n",
    "    base_model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE_MODEL_PATH, convert_from_hf=True)\n",
    "    \n",
    "    # 예시로 'q_proj' (Query Projection) 레이어를 뜯어봅니다.\n",
    "    print_layer_info(base_model, \"linear\") # 혹은 'linear' 등\n",
    "    \n",
    "    # 2. LoRA Model 적용 및 구조 확인\n",
    "    print(\"\\n\\n🟠 [2] LoRA Model (래핑 후) 로드 중...\")\n",
    "    # LoRAModel.from_pretrained는 내부적으로 base_model을 변형(In-place modification)합니다.\n",
    "    lora_model = PaddleOCRVLForConditionalGeneration.from_pretrained(LORA_PATH)\n",
    "    \n",
    "    # 동일한 레이어가 어떻게 변했는지 확인\n",
    "    print_layer_info(lora_model, \"linear\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da80961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 분석 시작\n",
      "📂 파일 로딩 시도: PaddlePaddle/PaddleOCR-VL/model.safetensors\n",
      "📂 파일 로딩 시도: ./PaddleOCR-VL-SFT-test06/model_state.pdparams\n",
      "\n",
      "📊 요약:\n",
      "   - Base 파일: model.safetensors (620 keys)\n",
      "   - Target 파일: model_state.pdparams (620 keys)\n",
      "\n",
      "✅ 키 목록이 완벽하게 일치합니다!\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 경로 설정\n",
    "BASE_MODEL_DIR = BASE\n",
    "TARGET_MODEL_DIR = \"./PaddleOCR-VL-SFT-test06\"\n",
    "\n",
    "def find_weight_file(directory):\n",
    "    \"\"\"폴더 내에서 가중치 파일을 찾습니다.\"\"\"\n",
    "    candidates = [\"model_state.pdparams\", \"model.pdparams\", \"model.safetensors\"]\n",
    "    for fname in candidates:\n",
    "        full_path = os.path.join(directory, fname)\n",
    "        if os.path.exists(full_path):\n",
    "            return full_path\n",
    "    return None\n",
    "\n",
    "def load_keys(path):\n",
    "    if not path:\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"📂 파일 로딩 시도: {path}\")\n",
    "    try:\n",
    "        # 확장자에 따라 로드 방식 분기\n",
    "        if path.endswith(\".safetensors\"):\n",
    "            from safetensors.numpy import load_file\n",
    "            # safetensors는 numpy dict로 반환됨\n",
    "            state_dict = load_file(path)\n",
    "            # 텐서로 변환이 필요할 수 있으나 키 비교 목적이면 그대로 사용 가능\n",
    "        else:\n",
    "            state_dict = paddle.load(path)\n",
    "            \n",
    "        return set(state_dict.keys()), state_dict\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 로드 에러: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def analyze_diff():\n",
    "    print(f\"🔍 분석 시작\")\n",
    "    \n",
    "    # 파일 찾기\n",
    "    base_file = find_weight_file(BASE_MODEL_DIR)\n",
    "    target_file = find_weight_file(TARGET_MODEL_DIR)\n",
    "    \n",
    "    if not base_file:\n",
    "        print(f\"❌ Base 모델 폴더({BASE_MODEL_DIR})에서 가중치 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    if not target_file:\n",
    "        print(f\"❌ Target 모델 폴더({TARGET_MODEL_DIR})에서 가중치 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 로드\n",
    "    base_keys, base_dict = load_keys(base_file)\n",
    "    target_keys, target_dict = load_keys(target_file)\n",
    "\n",
    "    if base_keys is None or target_keys is None:\n",
    "        print(\"❌ 가중치 로드에 실패하여 분석을 중단합니다.\")\n",
    "        return\n",
    "\n",
    "    # [비교 로직은 동일]\n",
    "    missing_in_target = base_keys - target_keys\n",
    "    extra_in_target = target_keys - base_keys\n",
    "\n",
    "    print(f\"\\n📊 요약:\")\n",
    "    print(f\"   - Base 파일: {os.path.basename(base_file)} ({len(base_keys)} keys)\")\n",
    "    print(f\"   - Target 파일: {os.path.basename(target_file)} ({len(target_keys)} keys)\")\n",
    "    \n",
    "    # 불필요한 키 (Extra Keys) - 에러의 주범\n",
    "    if extra_in_target:\n",
    "        print(f\"\\n🚨 [Extra Keys] Target에만 존재하는 불필요한 키 ({len(extra_in_target)}개):\")\n",
    "        sorted_extras = sorted(list(extra_in_target))\n",
    "        for i, k in enumerate(sorted_extras):\n",
    "            if i < 20: \n",
    "                # shape 정보 출력 시도\n",
    "                shape = \"Unknown\"\n",
    "                if hasattr(target_dict[k], 'shape'): shape = target_dict[k].shape\n",
    "                elif isinstance(target_dict[k], dict): shape = \"Dict?\" # safetensors numpy 등\n",
    "                else: shape = getattr(target_dict[k], 'shape', 'Unknown')\n",
    "                \n",
    "                print(f\"   ➕ {k} | Shape: {shape}\")\n",
    "            else:\n",
    "                print(f\"   ... (총 {len(extra_in_target)}개)\")\n",
    "                break\n",
    "        \n",
    "        # 분석 메시지\n",
    "        if any(\"bias\" in k for k in extra_in_target):\n",
    "            print(\"\\n   👉 [진단] 'bias'가 포함된 키가 다수 발견되었습니다.\")\n",
    "            print(\"       -> LoRA 병합 시 원래 bias=False인 레이어에 bias가 생성된 것으로 보입니다.\")\n",
    "            print(\"       -> 'fix_checkpoint.py'를 사용하여 이 키들을 삭제해야 합니다.\")\n",
    "            \n",
    "    elif not missing_in_target:\n",
    "        print(\"\\n✅ 키 목록이 완벽하게 일치합니다!\")\n",
    "    \n",
    "    # 누락된 키\n",
    "    if missing_in_target:\n",
    "        print(f\"\\n⚠️ [Missing Keys] Target에서 사라진 키 ({len(missing_in_target)}개):\")\n",
    "        for k in list(missing_in_target)[:10]:\n",
    "            print(f\"   ➖ {k}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc5336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1125 10:11:55.174090 11679 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-25 10:11:56,726] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:56,727] [    INFO]\u001b[0m - Loading configuration file /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:56,728] [    INFO]\u001b[0m - Loading weights file /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,269] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,438] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,442] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,449] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,454] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,459] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,463] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,470] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,474] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,478] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,487] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,491] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,497] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,506] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,511] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,523] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,528] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,540] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:11:59,546] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[33m[2025-11-25 10:12:00,041] [ WARNING]\u001b[0m - Some weights of the model checkpoint at /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged were not used when initializing PaddleOCRVLForConditionalGeneration: ['model.layers.0.mlp.down_proj.lora_A', 'model.layers.0.mlp.down_proj.lora_B', 'model.layers.0.mlp.up_gate_proj.weight', 'model.layers.0.self_attn.o_proj.lora_A', 'model.layers.0.self_attn.o_proj.lora_B', 'model.layers.0.self_attn.qkv_proj.weight', 'model.layers.1.mlp.down_proj.lora_A', 'model.layers.1.mlp.down_proj.lora_B', 'model.layers.1.mlp.up_gate_proj.weight', 'model.layers.1.self_attn.o_proj.lora_A', 'model.layers.1.self_attn.o_proj.lora_B', 'model.layers.1.self_attn.qkv_proj.weight', 'model.layers.10.mlp.down_proj.lora_A', 'model.layers.10.mlp.down_proj.lora_B', 'model.layers.10.mlp.up_gate_proj.weight', 'model.layers.10.self_attn.o_proj.lora_A', 'model.layers.10.self_attn.o_proj.lora_B', 'model.layers.10.self_attn.qkv_proj.weight', 'model.layers.11.mlp.down_proj.lora_A', 'model.layers.11.mlp.down_proj.lora_B', 'model.layers.11.mlp.up_gate_proj.weight', 'model.layers.11.self_attn.o_proj.lora_A', 'model.layers.11.self_attn.o_proj.lora_B', 'model.layers.11.self_attn.qkv_proj.weight', 'model.layers.12.mlp.down_proj.lora_A', 'model.layers.12.mlp.down_proj.lora_B', 'model.layers.12.mlp.up_gate_proj.weight', 'model.layers.12.self_attn.o_proj.lora_A', 'model.layers.12.self_attn.o_proj.lora_B', 'model.layers.12.self_attn.qkv_proj.weight', 'model.layers.13.mlp.down_proj.lora_A', 'model.layers.13.mlp.down_proj.lora_B', 'model.layers.13.mlp.up_gate_proj.weight', 'model.layers.13.self_attn.o_proj.lora_A', 'model.layers.13.self_attn.o_proj.lora_B', 'model.layers.13.self_attn.qkv_proj.weight', 'model.layers.14.mlp.down_proj.lora_A', 'model.layers.14.mlp.down_proj.lora_B', 'model.layers.14.mlp.up_gate_proj.weight', 'model.layers.14.self_attn.o_proj.lora_A', 'model.layers.14.self_attn.o_proj.lora_B', 'model.layers.14.self_attn.qkv_proj.weight', 'model.layers.15.mlp.down_proj.lora_A', 'model.layers.15.mlp.down_proj.lora_B', 'model.layers.15.mlp.up_gate_proj.weight', 'model.layers.15.self_attn.o_proj.lora_A', 'model.layers.15.self_attn.o_proj.lora_B', 'model.layers.15.self_attn.qkv_proj.weight', 'model.layers.16.mlp.down_proj.lora_A', 'model.layers.16.mlp.down_proj.lora_B', 'model.layers.16.mlp.up_gate_proj.weight', 'model.layers.16.self_attn.o_proj.lora_A', 'model.layers.16.self_attn.o_proj.lora_B', 'model.layers.16.self_attn.qkv_proj.weight', 'model.layers.17.mlp.down_proj.lora_A', 'model.layers.17.mlp.down_proj.lora_B', 'model.layers.17.mlp.up_gate_proj.weight', 'model.layers.17.self_attn.o_proj.lora_A', 'model.layers.17.self_attn.o_proj.lora_B', 'model.layers.17.self_attn.qkv_proj.weight', 'model.layers.2.mlp.down_proj.lora_A', 'model.layers.2.mlp.down_proj.lora_B', 'model.layers.2.mlp.up_gate_proj.weight', 'model.layers.2.self_attn.o_proj.lora_A', 'model.layers.2.self_attn.o_proj.lora_B', 'model.layers.2.self_attn.qkv_proj.weight', 'model.layers.3.mlp.down_proj.lora_A', 'model.layers.3.mlp.down_proj.lora_B', 'model.layers.3.mlp.up_gate_proj.weight', 'model.layers.3.self_attn.o_proj.lora_A', 'model.layers.3.self_attn.o_proj.lora_B', 'model.layers.3.self_attn.qkv_proj.weight', 'model.layers.4.mlp.down_proj.lora_A', 'model.layers.4.mlp.down_proj.lora_B', 'model.layers.4.mlp.up_gate_proj.weight', 'model.layers.4.self_attn.o_proj.lora_A', 'model.layers.4.self_attn.o_proj.lora_B', 'model.layers.4.self_attn.qkv_proj.weight', 'model.layers.5.mlp.down_proj.lora_A', 'model.layers.5.mlp.down_proj.lora_B', 'model.layers.5.mlp.up_gate_proj.weight', 'model.layers.5.self_attn.o_proj.lora_A', 'model.layers.5.self_attn.o_proj.lora_B', 'model.layers.5.self_attn.qkv_proj.weight', 'model.layers.6.mlp.down_proj.lora_A', 'model.layers.6.mlp.down_proj.lora_B', 'model.layers.6.mlp.up_gate_proj.weight', 'model.layers.6.self_attn.o_proj.lora_A', 'model.layers.6.self_attn.o_proj.lora_B', 'model.layers.6.self_attn.qkv_proj.weight', 'model.layers.7.mlp.down_proj.lora_A', 'model.layers.7.mlp.down_proj.lora_B', 'model.layers.7.mlp.up_gate_proj.weight', 'model.layers.7.self_attn.o_proj.lora_A', 'model.layers.7.self_attn.o_proj.lora_B', 'model.layers.7.self_attn.qkv_proj.weight', 'model.layers.8.mlp.down_proj.lora_A', 'model.layers.8.mlp.down_proj.lora_B', 'model.layers.8.mlp.up_gate_proj.weight', 'model.layers.8.self_attn.o_proj.lora_A', 'model.layers.8.self_attn.o_proj.lora_B', 'model.layers.8.self_attn.qkv_proj.weight', 'model.layers.9.mlp.down_proj.lora_A', 'model.layers.9.mlp.down_proj.lora_B', 'model.layers.9.mlp.up_gate_proj.weight', 'model.layers.9.self_attn.o_proj.lora_A', 'model.layers.9.self_attn.o_proj.lora_B', 'model.layers.9.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.0.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.1.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.10.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.11.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.12.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.13.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.13.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.14.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.15.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.16.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.17.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.18.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.19.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.2.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.20.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.20.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.21.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.22.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.23.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.24.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.25.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.26.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.3.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.4.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.5.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.6.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.7.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.8.self_attn.qkv_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.qkv_proj.bias', 'visual.vision_model.encoder.layers.9.self_attn.qkv_proj.weight']\n",
      "- This IS expected if you are initializing PaddleOCRVLForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PaddleOCRVLForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2025-11-25 10:12:00,042] [ WARNING]\u001b[0m - Some weights of PaddleOCRVLForConditionalGeneration were not initialized from the model checkpoint at /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged and are newly initialized: ['model.layers.12.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.layers.16.mlp.up_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.layers.10.mlp.up_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.layers.0.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'model.layers.5.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.layers.11.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'model.layers.13.mlp.up_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.layers.6.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'visual.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'visual.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.layers.8.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'visual.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'visual.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'visual.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'visual.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'visual.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'visual.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.14.mlp.up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:12:00,044] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-25 10:12:00,048] [    INFO]\u001b[0m - Loading configuration file /home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-25 10:12:00,098] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "#BASE = \"PaddlePaddle/PaddleOCR-VL\"                     # 학습 베이스와 동일\n",
    "BASE = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged\"     # LoRA 출력 디렉터리(ERNIE 포맷)\n",
    "\n",
    "# 1) 베이스 로드(HF->Paddle 변환 필요 시)\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(BASE, padding_side=\"right\", model_max_length=4096)\n",
    "image_preprocess = SiglipImageProcessor.from_pretrained(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09345aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaddleOCRVLForConditionalGeneration(\n",
       "  (mlp_AR): Projector(\n",
       "    (pre_norm): LayerNorm(normalized_shape=[1152], epsilon=1e-05)\n",
       "    (linear_1): Linear(in_features=4608, out_features=4608, dtype=bfloat16)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4608, out_features=1024, dtype=bfloat16)\n",
       "  )\n",
       "  (visual): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2D(3, 1152, kernel_size=[14, 14], stride=[14, 14], padding=VALID, data_format=NCHW)\n",
       "        (position_embedding): Embedding(729, 1152, sparse=False, scale_grad_by_freq=False, dtype=paddle.bfloat16)\n",
       "        (packing_position_embedding): Embedding(32768, 1152, sparse=False, scale_grad_by_freq=False, dtype=paddle.bfloat16)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): LayerList(\n",
       "          (0): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (1): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (2): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (3): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (4): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (5): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (6): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (7): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (8): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (9): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (10): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (11): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (12): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (13): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (14): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (15): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (16): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (17): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (18): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (19): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (20): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (21): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (22): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (23): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (24): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (25): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (26): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rotary_pos_emb): SigLIPRotaryEmbedding()\n",
       "      )\n",
       "      (post_layernorm): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "      (head): SiglipMultiheadAttentionPoolingHead(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "        )\n",
       "        (layernorm): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Ernie4_5Model(\n",
       "    (embed_tokens): Embedding(103424, 1024, sparse=False, scale_grad_by_freq=False, dtype=paddle.bfloat16)\n",
       "    (layers): LayerList(\n",
       "      (0): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (1): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (2): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (3): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (4): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (5): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (6): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (7): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (8): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (9): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (10): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (11): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (12): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (13): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (14): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (15): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (16): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (17): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (rotary_emb): KeyeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=103424, dtype=bfloat16)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ef72ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaddleOCRVLForConditionalGeneration(\n",
       "  (mlp_AR): Projector(\n",
       "    (pre_norm): LayerNorm(normalized_shape=[1152], epsilon=1e-05)\n",
       "    (linear_1): Linear(in_features=4608, out_features=4608, dtype=bfloat16)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4608, out_features=1024, dtype=bfloat16)\n",
       "  )\n",
       "  (visual): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2D(3, 1152, kernel_size=[14, 14], stride=[14, 14], padding=VALID, data_format=NCHW)\n",
       "        (position_embedding): Embedding(729, 1152, sparse=False, scale_grad_by_freq=False, dtype=paddle.bfloat16)\n",
       "        (packing_position_embedding): Embedding(32768, 1152, sparse=False, scale_grad_by_freq=False, dtype=paddle.bfloat16)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): LayerList(\n",
       "          (0): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (1): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (2): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (3): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (4): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (5): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (6): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (7): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (8): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (9): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (10): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (11): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (12): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (13): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (14): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (15): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (16): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (17): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (18): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (19): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (20): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (21): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (22): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (23): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (24): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (25): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "          (26): SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rotary_pos_emb): SigLIPRotaryEmbedding()\n",
       "      )\n",
       "      (post_layernorm): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "      (head): SiglipMultiheadAttentionPoolingHead(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (out_proj): Linear(in_features=1152, out_features=1152, dtype=bfloat16)\n",
       "        )\n",
       "        (layernorm): LayerNorm(normalized_shape=[1152], epsilon=1e-06)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, dtype=bfloat16)\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, dtype=bfloat16)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Ernie4_5Model(\n",
       "    (embed_tokens): Embedding(103424, 1024, sparse=False, scale_grad_by_freq=False, dtype=paddle.bfloat16)\n",
       "    (layers): LayerList(\n",
       "      (0): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (1): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (2): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (3): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (4): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (5): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (6): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (7): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (8): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (9): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (10): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (11): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (12): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (13): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (14): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (15): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (16): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "      (17): Ernie4_5DecoderLayer(\n",
       "        (self_attn): Ernie4_5Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, dtype=bfloat16)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, dtype=bfloat16)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (mlp): Ernie4_5MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, dtype=bfloat16)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, dtype=bfloat16)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (residual_add1): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (residual_add2): FusedDropoutImpl(\n",
       "          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (rotary_emb): KeyeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=103424, dtype=bfloat16)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f361267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-24 18:20:59 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 11-24 18:21:01 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 11-24 18:21:01 [__init__.py:32] name=register_paddlex_genai_models, value=paddlex.inference.genai.backends.vllm:register_models\n",
      "INFO 11-24 18:21:01 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 11-24 18:21:01 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 11-24 18:21:05 [__init__.py:44] plugin register_paddlex_genai_models loaded.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AttrsDescriptor' from 'triton.compiler.compiler' (/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/triton/compiler/compiler.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_inductor/runtime/hints.py:46\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttrsDescriptor\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mAttrsDescriptorWrapper\u001b[39m(\n\u001b[32m     49\u001b[39m         divisible_by_16=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     50\u001b[39m         equal_to_1=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     51\u001b[39m     ):\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Prepare the arguments for AttrsDescriptor\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AttrsDescriptor' from 'triton.backends.compiler' (/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/triton/backends/compiler.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhome/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py:217\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     compilation_config_instance = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m engine_args = \u001b[43mEngineArgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_local_media_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_local_media_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mswap_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswap_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcpu_offload_gb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_custom_all_reduce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_custom_all_reduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_async_output_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_async_output_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverride_pooler_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverride_pooler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompilation_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompilation_config_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = LLMEngine.from_engine_args(\n\u001b[32m    248\u001b[39m     engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:107\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model, served_model_name, tokenizer, hf_config_path, task, skip_tokenizer_init, tokenizer_mode, trust_remote_code, allowed_local_media_path, download_dir, load_format, config_format, dtype, kv_cache_dtype, seed, max_model_len, distributed_executor_backend, pipeline_parallel_size, tensor_parallel_size, data_parallel_size, enable_expert_parallel, max_parallel_loading_workers, block_size, enable_prefix_caching, prefix_caching_hash_algo, disable_sliding_window, disable_cascade_attn, use_v2_block_manager, swap_space, cpu_offload_gb, gpu_memory_utilization, max_num_batched_tokens, max_num_partial_prefills, max_long_partial_prefills, long_prefill_token_threshold, max_num_seqs, max_logprobs, disable_log_stats, revision, code_revision, rope_scaling, rope_theta, hf_token, hf_overrides, tokenizer_revision, quantization, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, tokenizer_pool_size, tokenizer_pool_type, tokenizer_pool_extra_config, limit_mm_per_prompt, mm_processor_kwargs, disable_mm_preprocessor_cache, enable_lora, enable_lora_bias, max_loras, max_lora_rank, fully_sharded_loras, max_cpu_loras, lora_dtype, lora_extra_vocab_size, long_lora_scaling_factors, enable_prompt_adapter, max_prompt_adapters, max_prompt_adapter_token, device, num_scheduler_steps, multi_step_stream_outputs, ray_workers_use_nsight, num_gpu_blocks_override, num_lookahead_slots, model_loader_extra_config, ignore_patterns, preemption_mode, scheduler_delay_factor, enable_chunked_prefill, disable_chunked_mm_input, guided_decoding_backend, logits_processor_pattern, speculative_config, qlora_adapter_name_or_path, show_hidden_metrics_for_version, otlp_traces_endpoint, collect_detailed_traces, disable_async_output_proc, scheduling_policy, scheduler_cls, override_neuron_config, override_pooler_config, compilation_config, worker_cls, worker_extension_cls, kv_transfer_config, generation_config, override_generation_config, enable_sleep_mode, model_impl, calculate_kv_scales, additional_config, enable_reasoning, reasoning_parser, use_tqdm_on_load)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/arg_utils.py:357\u001b[39m, in \u001b[36mEngineArgs.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# Setup plugins\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_general_plugins\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m \u001b[43mload_general_plugins\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/plugins/__init__.py:82\u001b[39m, in \u001b[36mload_general_plugins\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# general plugins, we only need to execute the loaded functions\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m plugins.values():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddlex/inference/genai/backends/vllm.py:30\u001b[39m, in \u001b[36mregister_models\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m ALL_MODEL_NAMES:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ModelRegistry.get_supported_archs():\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         net_cls, _ = \u001b[43mget_model_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvllm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m         ModelRegistry.register_model(net_cls.\u001b[34m__name__\u001b[39m, net_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddlex/inference/genai/models/__init__.py:72\u001b[39m, in \u001b[36mget_model_components\u001b[39m\u001b[34m(model_name, backend)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m network_class = \u001b[43m_get_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNETWORK_CLASS_GETTER_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33msglang\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     75\u001b[39m     processor_class = _get_component(PROCESSOR_CLASS_GETTER_KEY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddlex/inference/genai/models/__init__.py:60\u001b[39m, in \u001b[36mget_model_components.<locals>._get_component\u001b[39m\u001b[34m(getter_key)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` does not have `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgetter_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m getter = \u001b[38;5;28mgetattr\u001b[39m(model_module, getter_key)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m comp = \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m comp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddlex/inference/genai/models/paddleocr_vl_09b/__init__.py:18\u001b[39m, in \u001b[36mget_network_class\u001b[39m\u001b[34m(backend)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_network_class\u001b[39m(backend):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mvllm\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_vllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PaddleOCRVLForConditionalGeneration\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m PaddleOCRVLForConditionalGeneration\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33msglang\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddlex/inference/genai/models/paddleocr_vl_09b/_vllm.py:46\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_act_fn\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     ColumnParallelLinear,\n\u001b[32m     43\u001b[39m     QKVParallelLinear,\n\u001b[32m     44\u001b[39m     RowParallelLinear,\n\u001b[32m     45\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogits_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogitsProcessor\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantizationConfig\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvocab_parallel_embedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelLMHead\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (tensor_model_parallel_all_gather,\n\u001b[32m     12\u001b[39m                               tensor_model_parallel_gather)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvocab_parallel_embedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     VocabParallelEmbedding)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SamplingMetadata\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py:139\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_org_elements <= \u001b[38;5;28mself\u001b[39m.num_org_elements_padded\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_added_elements <= \u001b[38;5;28mself\u001b[39m.num_added_elements_padded\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_platform\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimple_compile_backend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mget_masked_input_and_mask\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morg_vocab_start_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43morg_vocab_end_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_org_vocab_padding\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43madded_vocab_start_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43madded_vocab_end_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mTuple\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# torch.compile will fuse all of the pointwise ops below\u001b[39;49;00m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# into a single kernel, making it very fast\u001b[39;49;00m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43morg_vocab_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43morg_vocab_start_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43morg_vocab_end_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43madded_vocab_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_vocab_start_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_vocab_end_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/__init__.py:2536\u001b[39m, in \u001b[36mcompile.<locals>.fn\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m   2534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be None\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2536\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/__init__.py:2565\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[39m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2563\u001b[39m     backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dynamo\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnopython\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:842\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnopython\u001b[39m\u001b[33m\"\u001b[39m] = ca_kwargs_override[\u001b[33m\"\u001b[39m\u001b[33mfullgraph\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrebuild_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:896\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(rebuild_ctx, backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[39m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    890\u001b[39m     disable\n\u001b[32m    891\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTORCHDYNAMO_DISABLE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    892\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m justknobs_check(\u001b[33m\"\u001b[39m\u001b[33mpytorch/compiler:enable_dynamo\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    893\u001b[39m ):\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _NullDecorator()\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m backend = \u001b[43mget_compiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[38;5;66;03m# Find if backend has any extra context manager\u001b[39;00m\n\u001b[32m    899\u001b[39m backend_ctx_ctor = \u001b[38;5;28mgetattr\u001b[39m(backend, \u001b[33m\"\u001b[39m\u001b[33mbackend_ctx_ctor\u001b[39m\u001b[33m\"\u001b[39m, null_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:783\u001b[39m, in \u001b[36mget_compiler_fn\u001b[39m\u001b[34m(compiler_fn)\u001b[39m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_compiler_fn\u001b[39m(compiler_fn):\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrepro\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mafter_dynamo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_backend_debug\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiler_fn, \u001b[33m\"\u001b[39m\u001b[33mcompiler_name\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    786\u001b[39m         compiler_str = compiler_fn.compiler_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompiledFn\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     AccuracyError,\n\u001b[32m     18\u001b[39m     backend_accuracy_fails,\n\u001b[32m     19\u001b[39m     BUCK_CMD_PREFIX,\n\u001b[32m     20\u001b[39m     BuckTargetWriter,\n\u001b[32m     21\u001b[39m     extra_imports,\n\u001b[32m     22\u001b[39m     generate_config_string,\n\u001b[32m     23\u001b[39m     helper_for_dump_minify,\n\u001b[32m     24\u001b[39m     InputReader,\n\u001b[32m     25\u001b[39m     InputWriter,\n\u001b[32m     26\u001b[39m     minifier_dir,\n\u001b[32m     27\u001b[39m     NNModuleToString,\n\u001b[32m     28\u001b[39m     NopInputReader,\n\u001b[32m     29\u001b[39m     run_fwd_maybe_bwd,\n\u001b[32m     30\u001b[39m     same_two_models,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx_placeholder_targets\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/debug_utils.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeta_utils\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtesting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rand_strided\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_float_dtype\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreductions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/testing.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebugging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aot_eager\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, eval_frame, optimize_assert, reset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_dynamo/backends/debugging.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m min_cut_rematerialization_partition\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _guards\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m functorch_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/functorch/compile/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maot_autograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     aot_function,\n\u001b[32m      4\u001b[39m     aot_module,\n\u001b[32m      5\u001b[39m     aot_module_simplified,\n\u001b[32m      6\u001b[39m     compiled_function,\n\u001b[32m      7\u001b[39m     compiled_module,\n\u001b[32m      8\u001b[39m     get_aot_compilation_context,\n\u001b[32m      9\u001b[39m     get_aot_graph_name,\n\u001b[32m     10\u001b[39m     get_graph_being_compiled,\n\u001b[32m     11\u001b[39m     make_boxed_compiler,\n\u001b[32m     12\u001b[39m     make_boxed_func,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompilers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     debug_compile,\n\u001b[32m     16\u001b[39m     default_decompositions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     ts_compile,\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx_minifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m minifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:36\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     dynamo_timed,\n\u001b[32m     32\u001b[39m     get_chromium_event_logger,\n\u001b[32m     33\u001b[39m     preserve_rng_state,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect_fake_mode\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_code\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutputCode\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BoxedBool, InputType\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor, FakeTensorMode\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_inductor/output_code.py:47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcudagraph_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     BoxedDeviceIndex,\n\u001b[32m     49\u001b[39m     CudagraphCachedInfo,\n\u001b[32m     50\u001b[39m     get_placeholder_info,\n\u001b[32m     51\u001b[39m     log_cudagraph_skip_and_bump_counter,\n\u001b[32m     52\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     54\u001b[39m     align_inputs_from_check_idxs,\n\u001b[32m     55\u001b[39m     BoxedBool,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     set_tracing_context_output_strides,\n\u001b[32m     59\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_inductor/cudagraph_utils.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputType\n\u001b[32m     13\u001b[39m perf_hint_log = torch._logging.getArtifactLogger(\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mperf_hints\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m static_inputs_log = torch._logging.getArtifactLogger(\n\u001b[32m     15\u001b[39m     \u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcudagraph_static_inputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_inductor/utils.py:50\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceProperties\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ELEMENTWISE_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/torch/_inductor/runtime/hints.py:67\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttrsDescriptor\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mAttrsDescriptorWrapper\u001b[39m(\n\u001b[32m     70\u001b[39m         divisible_by_16=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     71\u001b[39m         equal_to_1=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     72\u001b[39m     ):\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# Prepare the arguments for AttrsDescriptor\u001b[39;00m\n\u001b[32m     74\u001b[39m         kwargs = {\n\u001b[32m     75\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdivisible_by_16\u001b[39m\u001b[33m\"\u001b[39m: divisible_by_16,\n\u001b[32m     76\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mequal_to_1\u001b[39m\u001b[33m\"\u001b[39m: equal_to_1,\n\u001b[32m     77\u001b[39m         }\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AttrsDescriptor' from 'triton.compiler.compiler' (/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/triton/compiler/compiler.py)"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "llm = LLM(model=\"home/mango/ERNIE/PaddleOCR-VL-SFT-paddle-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7956cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1124 17:42:38.978982 254112 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-24 17:42:41,147] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:41,148] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:41,151] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,577] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,733] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,739] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,744] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,748] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,755] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,759] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,763] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,768] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,773] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,779] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,785] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,792] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,797] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,804] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,811] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,817] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,825] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:45,830] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:46,213] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:46,213] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:46,215] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:46,216] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-11-24 17:42:46,230] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:46,274] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-24 17:42:46,438] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[33m[2025-11-24 17:42:46,573] [ WARNING]\u001b[0m - model.generation_config is in conflict with model.config, model.config is used.\u001b[0m\n",
      "W1124 17:42:46.591598 254112 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\n",
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/decorator_utils.py:420: Warning: \n",
      "Non compatible API. Please refer to https://www.paddlepaddle.org.cn/documentation/docs/en/develop/guides/model_convert/convert_from_pytorch/api_difference/torch/torch.max.html first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image features and image tokens do not match: tokens: 0, features 390",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     49\u001b[39m inputs = {**txt_inputs, **vis_inputs}\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 5) 생성\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 중요: transformers처럼 **inputs 로 호출하면 TypeError가 납니다.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#       PaddleOCR-VL은 inputs=dict 를 첫 인자로 넘겨야 합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m gen_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# 6) 디코딩\u001b[39;00m\n\u001b[32m     57\u001b[39m text = tokenizer.decode(gen_ids.numpy().tolist(), skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/mango/ERNIE/ernie/modeling_paddleocr_vl.py:717\u001b[39m, in \u001b[36mPaddleOCRVLForConditionalGeneration.generate\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    715\u001b[39m gen_kwargs = {**inputs, **gen_kwargs}\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m paddle.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     generated_ids = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m generated_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/base/dygraph/base.py:405\u001b[39m, in \u001b[36m_DecoratorContextManager.__call__.<locals>._decorate_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decorate_function\u001b[39m(*args, **kwargs):\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/generation/utils.py:1056\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, input_ids, generation_config, stopping_criteria, streamer, synced_gpus, **kwargs)\u001b[39m\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generation_config.num_return_sequences > \u001b[32m1\u001b[39m:\n\u001b[32m   1052\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1053\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`num_return_sequences` has to be 1, but is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1054\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen doing greedy search.\u001b[39m\u001b[33m\"\u001b[39m.format(generation_config.num_return_sequences)\n\u001b[32m   1055\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfast_ptq_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfast_ptq_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrunc_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrunc_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_config.decode_strategy == \u001b[33m\"\u001b[39m\u001b[33msampling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generation_config.num_return_sequences > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddleformers/generation/utils.py:1210\u001b[39m, in \u001b[36mGenerationMixin.greedy_search\u001b[39m\u001b[34m(self, input_ids, logits_processors, max_length, pad_token_id, eos_token_id, stopping_criteria, streamer, fast_ptq_sampling, trunc_input, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   1207\u001b[39m \u001b[38;5;66;03m# prepare model inputs & get model output\u001b[39;00m\n\u001b[32m   1208\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1210\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m generate_end:\n\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/nn/layer/layers.py:1576\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1567\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *inputs: Any, **kwargs: Any) -> Any:\n\u001b[32m   1568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1569\u001b[39m         (\u001b[38;5;129;01mnot\u001b[39;00m in_to_static_mode())\n\u001b[32m   1570\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks)\n\u001b[32m   (...)\u001b[39m\u001b[32m   1574\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m in_profiler_mode() \u001b[38;5;129;01mor\u001b[39;00m in_sot_simulation_mode())\n\u001b[32m   1575\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1578\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dygraph_call_func(*inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/mango/ERNIE/ernie/modeling_paddleocr_vl.py:599\u001b[39m, in \u001b[36mPaddleOCRVLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inbatch_pack_offset, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, second_per_grid_ts, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m n_image_features = image_embeds.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_image_tokens != n_image_features:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    600\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage features and image tokens do not match: tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    601\u001b[39m     )\n\u001b[32m    603\u001b[39m mask = input_ids == \u001b[38;5;28mself\u001b[39m.config.image_token_id\n\u001b[32m    604\u001b[39m mask_unsqueezed = mask.unsqueeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Image features and image tokens do not match: tokens: 0, features 390"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paddle\n",
    "from PIL import Image\n",
    "\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "# 1) 경로\n",
    "BASE = \"PaddlePaddle/PaddleOCR-VL\"  # 학습 시 사용한 베이스와 동일\n",
    "LORA = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test05\"  # peft_model-*.safetensors + lora_config.json\n",
    "\n",
    "# 2) 베이스 모델/토크나이저/이미지 프로세서 로드\n",
    "#    HF 허브 베이스에서 Paddle로 변환 로드가 필요하면 convert_from_hf=True\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE, convert_from_hf=True)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(BASE, padding_side=\"right\", model_max_length=8192)\n",
    "image_processor = SiglipImageProcessor.from_pretrained(BASE)\n",
    "\n",
    "# 3) LoRA 어댑터 로드(ERNIE/Paddle 포맷 지원)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=LORA)\n",
    "model.eval()\n",
    "\n",
    "# 4) 입력 준비 (예: OCR 프롬프트)\n",
    "img_path = \"img-000000012.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image},\n",
    "        {\"type\": \"text\",  \"text\": \"OCR:\"}\n",
    "     ]}\n",
    "]\n",
    "\n",
    "# 텍스트 토크나이즈\n",
    "txt_inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pd\"\n",
    ")\n",
    "\n",
    "# 비전 전처리: pixel_values + image_grid_thw 필수\n",
    "vis_inputs = image_processor.preprocess(images=[image], return_tensors=\"pd\")\n",
    "\n",
    "# 모델 입력 합치기\n",
    "inputs = {**txt_inputs, **vis_inputs}\n",
    "\n",
    "# 5) 생성\n",
    "# 중요: transformers처럼 **inputs 로 호출하면 TypeError가 납니다.\n",
    "#       PaddleOCR-VL은 inputs=dict 를 첫 인자로 넘겨야 합니다.\n",
    "gen_ids = model.generate(inputs, max_new_tokens=1024, use_cache=True)[0]\n",
    "\n",
    "# 6) 디코딩\n",
    "text = tokenizer.decode(gen_ids.numpy().tolist(), skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867512f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PaddleOCRVLForConditionalGeneration.generate() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     31\u001b[39m messages = [\n\u001b[32m     32\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,         \n\u001b[32m     33\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     }\n\u001b[32m     38\u001b[39m ]\n\u001b[32m     39\u001b[39m inputs = processor.tokenizer.apply_chat_template(\n\u001b[32m     40\u001b[39m     messages, \n\u001b[32m     41\u001b[39m     tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m outputs = processor.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(outputs)\n",
      "\u001b[31mTypeError\u001b[39m: PaddleOCRVLForConditionalGeneration.generate() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "from paddlex.inference.models.doc_vlm.processors.paddleocr_vl._paddleocr_vl import PaddleOCRVLProcessor\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer  # paddlex 배포 토크나이저가 HF 포맷\n",
    "\n",
    "CHOSEN_TASK = \"ocr\"  # Options: 'ocr' | 'table' | 'chart' | 'formula'\n",
    "\n",
    "PROMPTS = {\n",
    "    \"ocr\": \"OCR:\",\n",
    "    \"table\": \"Table Recognition:\",\n",
    "    \"formula\": \"Formula Recognition:\",\n",
    "    \"chart\": \"Chart Recognition:\",\n",
    "}\n",
    "\n",
    "image_path = \"000000014_경남.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "model_dir = \"./PaddleOCR-VL-SFT-test02\"  # --vl_rec_model_dir와 동일한 구조\n",
    "image_processor = SiglipImageProcessor.from_pretrained(model_dir)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "processor = PaddleOCRVLProcessor(image_processor=image_processor, tokenizer=tokenizer)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",         \n",
    "     \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": PROMPTS[CHOSEN_TASK]},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True, \t\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pd\"\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "outputs = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3edddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Tensor(shape=[1, 59], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
       "       [[93988, 23374, 727  , 1708 , 468  , 2580 , 574  , 468  , 2580 , 1708 ,\n",
       "         518  , 1320 , 93966, 93937, 2848 , 93937, 2848 , 3003 , 2943 , 93950,\n",
       "         13079, 1421 , 93950, 12   , 7    , 93960, 7    , 10   , 488  , 93919,\n",
       "         3    , 93960, 10   , 9    , 17785, 6    , 11   , 12790, 11   , 8    ,\n",
       "         93976, 3    , 93951, 745  , 11722, 727  , 1708 , 468  , 679  , 574  ,\n",
       "         468  , 679  , 1708 , 468  , 93972, 2497 , 11051, 17547, 93919]]), 'position_ids': Tensor(shape=[1, 59], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
       "       [[0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58]]), 'attention_mask': Tensor(shape=[1, 1, 59, 59], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
       "       [[[[1, 0, 0, ..., 0, 0, 0],\n",
       "          [1, 1, 0, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1, ..., 1, 0, 0],\n",
       "          [1, 1, 1, ..., 1, 1, 0],\n",
       "          [1, 1, 1, ..., 1, 1, 1]]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673d3d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1111 13:59:25.386947  2271 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-11-11 13:59:28,203] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:28,203] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:28,205] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,475] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,679] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,684] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,688] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,695] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,699] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,703] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,708] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,716] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,721] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,728] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,732] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,737] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,740] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,749] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,753] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,758] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,766] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:32,771] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:33,139] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:33,140] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:33,145] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:33,146] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:33,186] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-11-11 13:59:33,317] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[33m[2025-11-11 13:59:33,438] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'types.SimpleNamespace' object has no attribute 'video_fps'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     23\u001b[39m image_preprocess = SiglipImageProcessor.from_pretrained(LORA)\n\u001b[32m     25\u001b[39m args = SimpleNamespace(\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# End2EndProcessor/하위 단계들이 요구하는 최소 필드\u001b[39;00m\n\u001b[32m     27\u001b[39m     batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     adaptive_max_imgtoken_rate=\u001b[33m\"\u001b[39m\u001b[33m0.6,0.3,0.1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m processor = \u001b[43mEnd2EndProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_preprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 3) 입력 구성 (채팅 템플릿은 내부에서 적용됨)\u001b[39;00m\n\u001b[32m     56\u001b[39m img = Image.open(IMG).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/mango/ERNIE/data_processor/steps/end2end_processing/processor.py:42\u001b[39m, in \u001b[36mEnd2EndProcessor.__init__\u001b[39m\u001b[34m(self, args, tokenizer, image_preprocess)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(args)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m.utterance_process = UtteranceProcessor(args, tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28mself\u001b[39m.coarse_processor = \u001b[43mCoarseProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids_massage_processor = InputIdsMassageProcessor(\n\u001b[32m     44\u001b[39m     args, tokenizer, image_preprocess\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.image_modification_processor = ImageModificationProcessor(\n\u001b[32m     47\u001b[39m     args, tokenizer, image_preprocess\n\u001b[32m     48\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/mango/ERNIE/data_processor/steps/coarse_processing/processor.py:49\u001b[39m, in \u001b[36mCoarseProcessor.__init__\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[32m     48\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(args)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28mself\u001b[39m.video_fps = \u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvideo_fps\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m.video_min_frames = args.video_min_frames\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m.video_max_frames = args.video_max_frames\n",
      "\u001b[31mAttributeError\u001b[39m: 'types.SimpleNamespace' object has no attribute 'video_fps'"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image\n",
    "\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from paddleformers.peft import LoRAModel\n",
    "\n",
    "from data_processor.steps.end2end_processing.processor import End2EndProcessor\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "\n",
    "BASE = \"PaddlePaddle/PaddleOCR-VL\"   # 학습에 사용한 베이스\n",
    "LORA = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-test02\"  # LoRA 디렉토리\n",
    "IMG = \"/000000014_경남.jpg\"\n",
    "\n",
    "# 1) 모델 로드 (Paddle/ERNIE)\n",
    "model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE, convert_from_hf=True)\n",
    "model = LoRAModel.from_pretrained(model=model, lora_path=LORA)\n",
    "model.eval()\n",
    "\n",
    "# 2) 프로세서(ERNIE 네이티브)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(LORA, padding_side=\"right\", model_max_length=4096)\n",
    "image_preprocess = SiglipImageProcessor.from_pretrained(LORA)\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    # End2EndProcessor/하위 단계들이 요구하는 최소 필드\n",
    "    batch_size=1,\n",
    "    load_args_from_api=False,\n",
    "    variable_resolution=1,\n",
    "    min_pixels=28*28*130,\n",
    "    max_pixels=28*28*1280,\n",
    "    spatial_conv_size=1,\n",
    "    im_prefix_length=64,\n",
    "    max_seq_length=4096,\n",
    "    sft_shift_by_one=False,\n",
    "    sft_replace_ids=True,\n",
    "    sft_image_rescale=True,\n",
    "    sft_image_normalize=True,\n",
    "    image_token_len=64,\n",
    "    image_dtype=\"float32\",\n",
    "    render_timestamp=False,\n",
    "    chat_template=\"ernie_vl\",\n",
    "    use_pic_id=False,\n",
    "    drop_untrainble_sample=True,\n",
    "    video_min_pixels=28*28*130,\n",
    "    video_max_pixels=28*28*1280,\n",
    "    corpus_name=\"inference\",\n",
    "    one_sample_in_one_seq=True,\n",
    "    adaptive_max_imgtoken_option=\"64,96,128\",\n",
    "    adaptive_max_imgtoken_rate=\"0.6,0.3,0.1\",\n",
    ")\n",
    "\n",
    "processor = End2EndProcessor(args, tokenizer, image_preprocess)\n",
    "\n",
    "# 3) 입력 구성 (채팅 템플릿은 내부에서 적용됨)\n",
    "img = Image.open(IMG).convert(\"RGB\")\n",
    "user_input = {\n",
    "    \"context\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"utterance\": [\n",
    "                {\"type\": \"text\", \"text\": \"OCR:\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": img}},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4) 텐서 생성\n",
    "rets = processor.process(user_input)\n",
    "ret = rets[0]\n",
    "\n",
    "# 학습 루프와 동일한 키로 변환\n",
    "inputs = {\n",
    "    \"input_ids\": paddle.to_tensor(ret[\"input_ids\"], dtype=\"int64\").unsqueeze(0),\n",
    "    \"token_type_ids\": paddle.to_tensor(ret[\"token_type_ids\"], dtype=\"int64\").unsqueeze(0),\n",
    "    \"image_type_ids\": paddle.to_tensor(ret[\"image_type_ids\"], dtype=\"int64\").unsqueeze(0),\n",
    "    \"pixel_values\": paddle.to_tensor(ret[\"images\"], dtype=\"float32\"),\n",
    "    \"image_grid_thw\": paddle.to_tensor(ret[\"grid_thw\"], dtype=\"int64\"),\n",
    "    \"position_ids\": paddle.to_tensor(ret[\"position_ids\"], dtype=\"int64\"),\n",
    "}\n",
    "\n",
    "# 학습 루프의 position_ids 변환 적용\n",
    "position_ids = inputs[\"position_ids\"].squeeze(0).transpose([1, 0]).unsqueeze(1)\n",
    "inputs[\"position_ids\"] = position_ids\n",
    "\n",
    "# 5) 생성 및 디코딩\n",
    "gen_ids = model.generate(**inputs, max_new_tokens=512, use_cache=True)[0]\n",
    "text = tokenizer.decode(gen_ids.numpy().tolist(), skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e5a804",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./PaddleOCR-VL-SFT-test02 does not appear to have a file named processing_ppocrvl.py. Checkout 'https://huggingface.co/./PaddleOCR-VL-SFT-test02/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m./PaddleOCR-VL-SFT-test02\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:376\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m    372\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m    373\u001b[39m     )\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     processor_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessor_auto_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    380\u001b[39m     processor_class.register_for_auto_class()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:570\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m     code_revision = revision\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload=force_download)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:372\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m new_files = []\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     resolved_module_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local \u001b[38;5;129;01mand\u001b[39;00m cached_module != resolved_module_file:\n\u001b[32m    386\u001b[39m         new_files.append(module_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/utils/hub.py:436\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[38;5;129;01mand\u001b[39;00m filename != os.path.join(subfolder, \u001b[33m\"\u001b[39m\u001b[33mconfig.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    435\u001b[39m     revision_ = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m revision\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    437\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    439\u001b[39m     )\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: ./PaddleOCR-VL-SFT-test02 does not appear to have a file named processing_ppocrvl.py. Checkout 'https://huggingface.co/./PaddleOCR-VL-SFT-test02/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_path = \"./PaddleOCR-VL-SFT-test02\"\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ab7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-DocLayoutV2`.\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000012.jpg', 'page_index': None, 'boxes': [{'cls_id': 22, 'label': 'text', 'score': 0.5718106031417847, 'coordinate': [0.15052554, 0.18856022, 1078.6244, 282.74127]}]}}\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000000.jpg', 'page_index': None, 'boxes': [{'cls_id': 7, 'label': 'figure_title', 'score': 0.5411680936813354, 'coordinate': [629.2026, 127.50459, 1020.1976, 179.55023]}, {'cls_id': 21, 'label': 'table', 'score': 0.9869638085365295, 'coordinate': [69.662895, 110.67651, 1577.9956, 2223.0688]}]}}\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000022.jpg', 'page_index': None, 'boxes': [{'cls_id': 17, 'label': 'paragraph_title', 'score': 0.8050800561904907, 'coordinate': [14.97492, 7.118814, 545.42346, 384.38373]}]}}\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from paddleocr import LayoutDetection\n",
    "\n",
    "#이미지 리스트로\n",
    "image_list = [\"img-000000012.jpg\", \"img-000000000.jpg\", \"img-000000022.jpg\"]\n",
    "model = LayoutDetection(model_name=\"PP-DocLayoutV2\", device=\"gpu\")\n",
    "output = model.predict(image_list, batch_size=2, layout_nms=True, threshold=0.5)\n",
    "for res in output:\n",
    "    res.print()\n",
    "    res.save_to_img(save_path=\"./output/\")\n",
    "    res.save_to_json(save_path=\"./output/res.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/myenv/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/RT-DETR-L_wired_table_cell_det`.\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000012.jpg', 'page_index': None, 'boxes': [{'cls_id': 0, 'label': 'cell', 'score': 0.4862610399723053, 'coordinate': [1.878852, 90.55357, 1079, 190.20425]}, {'cls_id': 0, 'label': 'cell', 'score': 0.46319085359573364, 'coordinate': [3.82787, 3.0447578, 1076.659, 90.90404]}, {'cls_id': 0, 'label': 'cell', 'score': 0.44285157322883606, 'coordinate': [187.59222, 189.90685, 1076.9583, 281.95325]}, {'cls_id': 0, 'label': 'cell', 'score': 0.41178473830223083, 'coordinate': [3.4190216, 190.72818, 214.80995, 282.13785]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3950755000114441, 'coordinate': [4.1579905, 190.00288, 74.28285, 281.3867]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3856847584247589, 'coordinate': [209.14444, 192.03714, 306.60007, 281.7777]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3856642544269562, 'coordinate': [214.34558, 91.23801, 305.15317, 190.9531]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3768150210380554, 'coordinate': [920.77203, 90.39845, 1077.7476, 189.8044]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3738648593425751, 'coordinate': [3.2324684, 2.698911, 74.11984, 89.74888]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3732838034629822, 'coordinate': [1.7412454, 1.2202984, 218.82256, 91.274574]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3699844777584076, 'coordinate': [6.3488975, 90.13699, 72.710754, 189.8414]}, {'cls_id': 0, 'label': 'cell', 'score': 0.36760661005973816, 'coordinate': [925.8167, 3.451777, 1077.8279, 89.97262]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3518524169921875, 'coordinate': [916.8864, 191.52266, 1077.3304, 281.78864]}, {'cls_id': 0, 'label': 'cell', 'score': 0.34620556235313416, 'coordinate': [215.85812, 2.0822296, 303.49594, 91.32001]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3373715281486511, 'coordinate': [69.0265, 89.90339, 162.72763, 189.43898]}, {'cls_id': 0, 'label': 'cell', 'score': 0.33525797724723816, 'coordinate': [304.7505, 190.12236, 519.8703, 281.36084]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3201681077480316, 'coordinate': [70.38399, 1.6085349, 163.32523, 90.275154]}, {'cls_id': 0, 'label': 'cell', 'score': 0.31902939081192017, 'coordinate': [518.7307, 90.40291, 737.2194, 189.15836]}, {'cls_id': 0, 'label': 'cell', 'score': 0.31411561369895935, 'coordinate': [741.97577, 4.478459, 926.8313, 90.453804]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3097003996372223, 'coordinate': [148.28633, 91.330284, 218.02579, 190.47025]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3089093863964081, 'coordinate': [734.71875, 91.3567, 922.7332, 189.52293]}, {'cls_id': 0, 'label': 'cell', 'score': 0.3084582984447479, 'coordinate': [730.01654, 190.04431, 917.037, 281.78943]}, {'cls_id': 0, 'label': 'cell', 'score': 0.30541691184043884, 'coordinate': [355.25635, 5.05755, 447.33902, 89.72558]}]}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import TableCellsDetection\n",
    "model = TableCellsDetection(model_name=\"RT-DETR-L_wired_table_cell_det\")\n",
    "output = model.predict(\"img-000000000.jpg\", threshold=0.3, batch_size=1)\n",
    "for res in output:\n",
    "    res.print(json_format=False)\n",
    "    res.save_to_img(\"./output/\")\n",
    "    res.save_to_json(\"./output/res.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b11eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50813/3833193561.py:3: DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n",
      "  ocr = PaddleOCR(use_angle_cls=False, lang='korean')\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-LCNet_x1_0_doc_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/UVDoc`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('korean_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/korean_PP-OCRv5_mobile_rec`.\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000012.jpg', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': False}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 0}, 'dt_polys': array([[[ 21,   0],\n",
      "        ...,\n",
      "        [ 21,  71]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[951, 235],\n",
      "        ...,\n",
      "        [951, 283]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0.0, 'return_word_box': False, 'rec_texts': ['.기', '간: 95. 1.- 95. 1.27', '조 사 반 :보호계장외 5명 (보호계', '직은', '대', '상 :허가지 18개소,복구지', '2개소'], 'rec_scores': array([0.78298801, ..., 0.99876738]), 'rec_polys': array([[[ 21,   0],\n",
      "        ...,\n",
      "        [ 21,  71]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[951, 235],\n",
      "        ...,\n",
      "        [951, 283]]], dtype=int16), 'rec_boxes': array([[ 21, ...,  71],\n",
      "       ...,\n",
      "       [951, ..., 283]], dtype=int16)}}\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000000.jpg', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': False}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 0}, 'dt_polys': array([[[ 621,   28],\n",
      "        ...,\n",
      "        [ 620,   83]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1312, 2293],\n",
      "        ...,\n",
      "        [1312, 2326]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0.0, 'return_word_box': False, 'rec_texts': ['BILL OF LADING', 'CONSIGNEE (provide complete name and address)', 'BOOKING NO.', 'BILL OF LADING NO.', 'FAE', 'SUNG', 'PRECISION CO., LTD.', '59552', 'HG133973', '295', 'PYONGJ-RI TAEAN-UP HWASONG-KUN,', 'GYEONGGI 13835, K0REA', 'EXPORT REFERENCES', 'TEL:+(65)(34)7963-6753 FAX:+(91)(17)4082-1952', '5742.01-0331', 'CONSIGNEE (please provide complete name and address)', 'FORWARDING AGENT / FMC NO.', 'FAPITAL BRAND CO., LTD.', 'NYCOMPUTERCAREER CO., LTD.', 'NO. 161, BAIMA LANE, CHANGSHA CITY, HUNAN,', '97748-0011-7147', 'CHINA 200150', 'TEL:+(28)(98)1257-1166 FAX:+(63)(27)5334-4371', 'COUNTRY OF ORIGIN CAMBODIA', 'NOTIFY PARTY (please provide complete name and address)', 'FOR DELIVERY OF GOODS PLEASE APPLY TO', 'FLOUDFIT SOFTWARE CO., LTD.', 'DHOMCHOM ROLLER', 'CO., LTD.', 'ROOM 350, SHIRENZI ROAD, XIUWEN COUNTY,', 'R0OM 652, XIADIAN', 'ROAD, ZHUOLU COUNTY,', 'SHAANXI, CHINA 910650', 'GUIZHOU, CHINA 910952', 'TEL:+(83)(80)6224-7954 FAX:+(62)(90)7181-6535', 'TEL:+(59)(21)5053-0520 FAX:+(77)(32)8548-5471', 'MODE OF INITIAL CARRIAGE', 'PLACE OF INITIAL RECEIPT', 'DOMESTIC ROUTING/EXPORT INSTRUCTIONS', 'FCA', 'ONEGLIA,', 'ITALY', 'YANGPYEONG KOREA', 'VESSEL NAME', 'PORT OF LOADING', 'SITC KOBE', 'CAMALTI,', 'TURKEY', 'FREIGHT PAYABLE AT', 'TYPE OF MOVEMENT', 'PORT OF DISCHARGE', 'OCEAN', 'PLACE OF DELIVERY BY CARRIER', 'MUSASHI,', 'JAPAN', 'SFAX,', 'TUNISIA', 'PARTICULARS FURNISHED BY SHIPPER', 'MARKS & NOS/CONT. NOS', 'NO. OF PACKAGES', 'DESCRIPTION OF PACKAGES AND GOODS', 'GROSS WEIGHT', 'MEASUREMENT', 'DLSU1958003', '53 PKG', 'DOOR', 'HANDLE', '345KG', '310.98', 'CBM', 'L216311', '72 PKG', 'VIBRATION ISOLATOR (FOR PRE /', '730KG', '996.27 CBM', 'POST TREATMENT EQUIPMENT)', 'TOTAL NUMBER OF PKGS.', '58 PKG', 'Liability Information', 'DECLARED VALUE: $', '335.52', \"Clause 20 on the reverse side hereof limits the carrier's liability to a maximum of Uss5oo per package or customary\", 'freight unit by incorporation of the Carriage of Goods by Sea Act. To protect for a higher value, you may declare a', \"If shipper enters a value, carrier's limitation of\", 'higher value and pay the ad valorem freight charge or purchase cargo insurance.', 'liability shall not apply and the ad valorem', 'Declared Value:', 'rate will be changed.', 'The shipper may increase the carrier\\'s liability by declaring a higher value in the \"Declared Value\" box to the right', 'and paying the additional charge that accompanies this.', 'SHIPPER REQUESTS INSURANCE:', 'Insurance:', 'Yes', 'No Must check one box!', 'The shipper may also purchase insurance on the goods listed on this bill of lading by indicating this in the box to the', '291.46', 'right and paying the additional premium.', 'Amount: $', 'FREIGHT RATES, CHARGES, WEIGHTS AND/OR MEASUREMENTS', 'REcEIVED FOR SHIPMENT from the MERCHANT in apparent good order and condition unless', 'otherwise stated herein, the GOoDS mentioned above to be transported as provided herein, by', 'PREPAID', 'COLLECT', 'any mode of transport for all or any part of the Carriage, SUBJEcT TO ALL THE TERMs AND', 'SUBJECT TO CORRECTIONS', \"CONDITIONS appearing on the face and back hereof and in the CARRIER'S applicable Tariff, to\", 'which the Merchant agrees by accepting this BlLL OF LADING.', 'THE B/L SHOULD BE DATED NOT', '$8,088.36', 'Where applicable law requires and not otherwise, one original Bill OF LADING must be', 'LATER THAN 2010-11-30', 'surrendered, duly endorsed, in exchange for the GOODS or CONTAINER(S) or other', \"PACKAGE(S), the others to stand void. If a 'Non-Negotiable' BILL OF LADING is issued, neither an\", 'original nor a copy need be surrendered in exchange for delivery unless applicable law so requires.', 'ALD AUTOMOTIVE CO., LTD.', 'BY', 'Km', 'AS CARRIER', '2014-10-20', 'DATED', 'JARRY'], 'rec_scores': array([0.98593837, ..., 0.98715574]), 'rec_polys': array([[[ 621,   28],\n",
      "        ...,\n",
      "        [ 620,   83]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1312, 2293],\n",
      "        ...,\n",
      "        [1312, 2326]]], dtype=int16), 'rec_boxes': array([[ 620, ...,   88],\n",
      "       ...,\n",
      "       [1312, ..., 2326]], dtype=int16)}}\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': 'img-000000022.jpg', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': False}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 180}, 'dt_polys': array([[[ 96,   0],\n",
      "        ...,\n",
      "        [ 98, 203]],\n",
      "\n",
      "       [[  4, 184],\n",
      "        ...,\n",
      "        [  8, 393]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, -1]), 'text_rec_score_thresh': 0.0, 'return_word_box': False, 'rec_texts': ['체크', '체크'], 'rec_scores': array([0.99971223, 0.97240543]), 'rec_polys': array([[[ 96,   0],\n",
      "        ...,\n",
      "        [ 98, 203]],\n",
      "\n",
      "       [[  4, 184],\n",
      "        ...,\n",
      "        [  8, 393]]], dtype=int16), 'rec_boxes': array([[ 96, ..., 203],\n",
      "       [  4, ..., 393]], dtype=int16)}}\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "image_list = [\"img-000000012.jpg\", \"img-000000000.jpg\", \"img-000000022.jpg\"]\n",
    "ocr = PaddleOCR(use_angle_cls=False, lang='korean')\n",
    "output = ocr.predict(image_list)\n",
    "#이미지 저장\n",
    "for res in output:\n",
    "    res.print()\n",
    "    res.save_to_img(\"output\")\n",
    "    res.save_to_json(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63acb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['doc_preprocessor_res']['angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a3d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9997122287750244, 0.9724054336547852]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output[0]['rec_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00450e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'list_available_plugins' from 'paddlex.inference' (/root/anaconda3/envs/myenv/lib/python3.9/site-packages/paddlex/inference/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpaddlex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_available_plugins\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(list_available_plugins())\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'list_available_plugins' from 'paddlex.inference' (/root/anaconda3/envs/myenv/lib/python3.9/site-packages/paddlex/inference/__init__.py)"
     ]
    }
   ],
   "source": [
    "from paddlex.inference import list_available_plugins\n",
    "print(list_available_plugins())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
