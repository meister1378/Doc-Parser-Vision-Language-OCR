# -----------environment args----------------------#
env:
    HOME: null

# ---------------------------model args-------------------------------------------------#
model_args:
    model_name_or_path: ../pre-training/model_configs/ERNIE-4p5-300B-A47B/
    tokenizer_name: ../pre-training/ernie/src/tokenizers/tokenizer_model
    output_dir: ./output/
    data_load_process_num: 40
    max_seq_length: 4096
    base_seq_length: 4096
    num_consecutive: 32
    sequence_parallel: 1
    enable_global_training_logs: False
    moe_use_aux_free_update_coef: 0.001
    global_logging_interval: 1
    model_config:
        multi_token_pred_depth: 0
        moe_logging: True
        moe_use_aux_free: True
        use_cache: False
        use_fast_ln: True
        fuse_gate_detach_matmul: False
        moe_group_orthogonal_loss: True
        moe_orthogonal_loss_lambda: 0.0
        moe_use_all2all: True


# ---------------------------trainer args-------------------------------------------------#
trainer_args:
    input_dir: "0.4 ../pre-training/demo_data/data-1-part0 0.6 ../pre-training/demo_data/data-1-part0"
    split: "998,1,1"
    use_sp_callback: True
    moe_gate_lr_ratio: 0.01
    do_train: True
    dataloader_num_workers: 8
    prefetch_factor: 32
    overwrite_output_dir: 1
    disable_tqdm: 1
    logging_steps: 1
    eval_steps: 1000
    eval_iters: -1
    save_steps: 100
    max_steps: 100
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8
    learning_rate: 2.2e-4
    min_lr: 2.2e-5
    global_batch_size: 49
    gradient_accumulation_steps: 7
    per_device_train_batch_size: 7
    per_device_eval_batch_size: 1
    lr_scheduler: wsd:231084
    decay_function: 1-sqrt
    max_grad_norm: 1.0

    use_async_save: True

    weight_decay: 0.1
    warmup_steps: 200
    save_total_limit: 5
    bf16: True
    fp16_opt_level: "O2"
    scale_loss: 4096
    seed: 666
    use_train_part_sharding: 1
    # pre_alloc_memory: 60
    offload_optimizer: True

    tensor_parallel_degree: 8
    pipeline_parallel_degree: 7
    virtual_pp_degree: 8
    pipeline_schedule_mode: "VPP"

    data_parallel_degree: 1
    sharding: "stage1"
    sharding_degree: 1
    amp_master_grad: 1
    pipeline_parallel_config: enable_delay_scale_loss
    sharding_parallel_config: split_param enable_fuse_optimizer_states
    sharding_comm_buffer_size_MB: 2048
    tensor_parallel_config: replace_with_parallel_cross_entropy

    skip_profile_timer: False
    ignore_data_skip: 0
    shuffle_consecutive: True
    load_sharded_model: True
    save_sharded_model: True
    ignore_load_lr_and_optim: False
    metrics_output_path: ./output/paddle_distributed_logs/

    use_moe: True
    moe_group: mp
    log_global_grad_norm: True
    enable_optimizer_timer: False
    gc_interval: 100000

    enable_auto_parallel: 1
    use_intermediate_api: False
    to_static: 0
