{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce83a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek-ocr/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "W1229 13:58:17.316450 14527 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.6, Runtime API Version: 11.8\n",
      "\u001b[32m[2025-12-29 13:58:22,033] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:22,034] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/config.json\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:22,036] [    INFO]\u001b[0m - Loading weights file PaddlePaddle/PaddleOCR-VL/model.safetensors\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,043] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,277] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,282] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,288] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,293] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,299] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,308] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,312] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,319] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,324] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,328] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,333] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,342] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,346] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,352] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,362] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,370] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,376] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,382] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,843] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,844] [    INFO]\u001b[0m - All the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at PaddlePaddle/PaddleOCR-VL.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,846] [    INFO]\u001b[0m - Using download source: huggingface\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,848] [    INFO]\u001b[0m - Loading configuration file PaddlePaddle/PaddleOCR-VL/generation_config.json\u001b[0m\n",
      "\u001b[33m[2025-12-29 13:58:27,860] [ WARNING]\u001b[0m - PretrainedTokenizer will be deprecated and removed in the next major release. Please migrate to Hugging Face's transformers.PreTrainedTokenizer. Checkout paddleformers/transformers/qwen/tokenizer.py for an example: use class QWenTokenizer(PaddleTokenizerMixin, hf.PreTrainedTokenizer) to support multisource download and Paddle tokenizer operations.\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:27,908] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:28,110] [    INFO]\u001b[0m - Load lora weight successfully\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:28,349] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000/config.json\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:28,352] [    INFO]\u001b[0m - Configuration saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000/generation_config.json\u001b[0m\n",
      "\u001b[32m[2025-12-29 13:58:34,749] [    INFO]\u001b[0m - Model weights saved in /home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000/model.safetensors\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from ernie.modeling_paddleocr_vl import PaddleOCRVLForConditionalGeneration\n",
    "from ernie.tokenizer import Ernie4_5_Tokenizer\n",
    "from data_processor.image_preprocessor.image_preprocessor_siglip import SiglipImageProcessor\n",
    "from paddleformers.peft import LoRAModel\n",
    "from paddleformers.peft import LoRAConfig, LoRAModel, PrefixConfig, PrefixModelForCausalLM\n",
    "from paddleformers.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from paddleformers.peft import LoRAAutoModel\n",
    "BASE = \"PaddlePaddle/PaddleOCR-VL\"                     # í•™ìŠµ ë² ì´ìŠ¤ì™€ ë™ì¼\n",
    "LORA = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000\"     # LoRA ì¶œë ¥ ë””ë ‰í„°ë¦¬(ERNIE í¬ë§·)\n",
    "\n",
    "# 1) ë² ì´ìŠ¤ ë¡œë“œ(HF->Paddle ë³€í™˜ í•„ìš” ì‹œ)\n",
    "base_model = PaddleOCRVLForConditionalGeneration.from_pretrained(BASE, convert_from_hf=True)\n",
    "tokenizer = Ernie4_5_Tokenizer.from_pretrained(LORA, padding_side=\"right\", model_max_length=4096)\n",
    "image_preprocess = SiglipImageProcessor.from_pretrained(LORA)\n",
    "\n",
    "#lora_config = LoRAConfig.from_pretrained(LORA)\n",
    "#dtype = lora_config.dtype\n",
    "#lora_config.merge_weights = True\n",
    "# 2) LoRA ì–´ëŒ‘í„° ì ìš©\n",
    "\n",
    "lora_model = LoRAModel.from_pretrained(base_model, lora_path=LORA)\n",
    "lora_model.merge()\n",
    "lora_model.restore_original_model()\n",
    "if hasattr(lora_model, \"model\"):\n",
    "    # LoRAModelì´ ê°ì‹¸ê³  ìˆëŠ” ì‹¤ì œ ëª¨ë¸ ê°ì²´ ì ‘ê·¼\n",
    "    merged_base_model = lora_model.model\n",
    "    #print(merged_base_model)\n",
    "    merged_base_model.save_pretrained(\n",
    "    LORA,\n",
    "    safe_serialization=True,   # HF safetensorsë¡œ ì €ì¥\n",
    ")\n",
    "#lora_model.model.save_pretrained(LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b9cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: /home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000/model.safetensors\n",
      "ğŸ”§ Transposing: lm_head.weight torch.Size([1024, 103424]) -> torch.Size([103424, 1024])\n",
      "ğŸ”§ Transposing: mlp_AR.linear_1.weight torch.Size([4608, 4608]) -> torch.Size([4608, 4608])\n",
      "ğŸ”§ Transposing: mlp_AR.linear_2.weight torch.Size([4608, 1024]) -> torch.Size([1024, 4608])\n",
      "ğŸ”§ Transposing: model.layers.0.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.0.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.0.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.0.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.0.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.0.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.0.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.1.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.1.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.1.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.1.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.1.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.1.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.1.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.10.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.10.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.10.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.10.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.10.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.10.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.10.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.11.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.11.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.11.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.11.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.11.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.11.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.11.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.12.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.12.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.12.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.12.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.12.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.12.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.12.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.13.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.13.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.13.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.13.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.13.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.13.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.13.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.14.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.14.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.14.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.14.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.14.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.14.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.14.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.15.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.15.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.15.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.15.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.15.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.15.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.15.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.16.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.16.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.16.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.16.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.16.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.16.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.16.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.17.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.17.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.17.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.17.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.17.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.17.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.17.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.2.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.2.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.2.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.2.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.2.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.2.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.2.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.3.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.3.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.3.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.3.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.3.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.3.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.3.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.4.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.4.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.4.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.4.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.4.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.4.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.4.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.5.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.5.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.5.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.5.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.5.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.5.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.5.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.6.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.6.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.6.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.6.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.6.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.6.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.6.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.7.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.7.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.7.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.7.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.7.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.7.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.7.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.8.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.8.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.8.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.8.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.8.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.8.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.8.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.9.mlp.down_proj.weight torch.Size([3072, 1024]) -> torch.Size([1024, 3072])\n",
      "ğŸ”§ Transposing: model.layers.9.mlp.gate_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.9.mlp.up_proj.weight torch.Size([1024, 3072]) -> torch.Size([3072, 1024])\n",
      "ğŸ”§ Transposing: model.layers.9.self_attn.k_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: model.layers.9.self_attn.o_proj.weight torch.Size([2048, 1024]) -> torch.Size([1024, 2048])\n",
      "ğŸ”§ Transposing: model.layers.9.self_attn.q_proj.weight torch.Size([1024, 2048]) -> torch.Size([2048, 1024])\n",
      "ğŸ”§ Transposing: model.layers.9.self_attn.v_proj.weight torch.Size([1024, 256]) -> torch.Size([256, 1024])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.0.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.0.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.1.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.1.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.10.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.10.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.11.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.11.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.12.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.12.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.12.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.12.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.12.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.12.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.13.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.13.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.13.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.13.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.13.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.13.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.14.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.14.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.14.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.14.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.14.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.14.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.15.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.15.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.15.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.15.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.15.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.15.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.16.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.16.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.16.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.16.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.16.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.16.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.17.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.17.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.17.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.17.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.17.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.17.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.18.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.18.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.18.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.18.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.18.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.18.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.19.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.19.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.19.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.19.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.19.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.19.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.2.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.2.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.20.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.20.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.20.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.20.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.20.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.20.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.21.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.21.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.21.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.21.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.21.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.21.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.22.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.22.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.22.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.22.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.22.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.22.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.23.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.23.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.23.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.23.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.23.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.23.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.24.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.24.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.24.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.24.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.24.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.24.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.25.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.25.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.25.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.25.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.25.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.25.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.26.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.26.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.26.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.26.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.26.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.26.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.3.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.3.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.4.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.4.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.5.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.5.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.6.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.6.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.7.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.7.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.8.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.8.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.9.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.9.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.head.attention.out_proj.weight torch.Size([1152, 1152]) -> torch.Size([1152, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.head.mlp.fc1.weight torch.Size([1152, 4304]) -> torch.Size([4304, 1152])\n",
      "ğŸ”§ Transposing: visual.vision_model.head.mlp.fc2.weight torch.Size([4304, 1152]) -> torch.Size([1152, 4304])\n",
      "\n",
      "ğŸ’¾ ì´ 294ê°œì˜ í…ì„œë¥¼ ë’¤ì§‘ì–´ ì €ì¥í•©ë‹ˆë‹¤...\n",
      "âœ… ì €ì¥ ì™„ë£Œ: /home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000/model_fixed.safetensors\n",
      "ğŸ‘‰ ì´ íŒŒì¼ì˜ ì´ë¦„ì„ 'model.safetensors'ë¡œ ë³€ê²½í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    from safetensors import safe_open\n",
    "    from safetensors.torch import save_file\n",
    "    import torch \n",
    "except ImportError:\n",
    "    print(\"âŒ 'pip install safetensors torch' í•„ìš”\")\n",
    "    exit()\n",
    "\n",
    "# =========================================================\n",
    "# [ì„¤ì •] ë¬¸ì œê°€ ìˆëŠ” LoRA ëª¨ë¸ ê²½ë¡œ\n",
    "# =========================================================\n",
    "TARGET_FILE = \"/home/mango/ERNIE/PaddleOCR-VL-SFT-OCRPUBLIC/checkpoint-250000/model.safetensors\"\n",
    "# =========================================================\n",
    "\n",
    "def fix_transposed_weights():\n",
    "    print(f\"ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: {TARGET_FILE}\")\n",
    "    \n",
    "    if not os.path.exists(TARGET_FILE):\n",
    "        print(\"âŒ íŒŒì¼ ì—†ìŒ\")\n",
    "        return\n",
    "\n",
    "    tensors_to_save = {}\n",
    "    fixed_count = 0\n",
    "    \n",
    "    with safe_open(TARGET_FILE, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for key in f.keys():\n",
    "            tensor = f.get_tensor(key)\n",
    "            \n",
    "            # 2D í…ì„œ(Linear Weight)ì¸ ê²½ìš° ì „ì¹˜ ëŒ€ìƒì¸ì§€ í™•ì¸\n",
    "            if len(tensor.shape) == 2:\n",
    "                # Transpose ëŒ€ìƒ í‚¤ì›Œë“œ í•„í„°ë§\n",
    "                if \"proj.weight\" in key or \"fc1.weight\" in key or \"fc2.weight\" in key or \"lm_head.weight\" in key or \"linear\" in key:\n",
    "                    print(f\"ğŸ”§ Transposing: {key} {tensor.shape} -> {tensor.t().shape}\")\n",
    "                    \n",
    "                    # [í•µì‹¬ ìˆ˜ì •] .t() ë’¤ì— .contiguous() ì¶”ê°€\n",
    "                    tensor = tensor.t().contiguous() \n",
    "                    \n",
    "                    fixed_count += 1\n",
    "            \n",
    "            tensors_to_save[key] = tensor\n",
    "\n",
    "    # 3. ì €ì¥\n",
    "    if fixed_count > 0:\n",
    "        print(f\"\\nğŸ’¾ ì´ {fixed_count}ê°œì˜ í…ì„œë¥¼ ë’¤ì§‘ì–´ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "        output_path = TARGET_FILE.replace(\".safetensors\", \"_fixed.safetensors\")\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì—†ì´ ì €ì¥ (ìˆœìˆ˜ í…ì„œ ë°ì´í„°)\n",
    "        save_file(tensors_to_save, output_path)\n",
    "        \n",
    "        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "        print(\"ğŸ‘‰ ì´ íŒŒì¼ì˜ ì´ë¦„ì„ 'model.safetensors'ë¡œ ë³€ê²½í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ìˆ˜ì •í•  í…ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fix_transposed_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
